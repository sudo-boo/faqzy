Question,Answer
 What is Data Science?,"Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights from it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.
Check out this comprehensive Data Scientist Certification!"
 Differentiate between Data Analytics and Data Science,"Data Analytics
Data Science


Data Analytics is a subset of Data Science.
Data Science is a broad technology that includes various subsets such as Data Analytics, Data Mining, Data Visualization, etc.


The goal of data analytics is to illustrate the precise details of retrieved insights.
The goal of data science is to discover meaningful insights from massive datasets and derive the best possible solutions to resolve business issues.


Requires just basic programming languages.
Requires knowledge in advanced programming languages, statistics, and special machine learning algorithms.


It focuses on just finding the solutions.
Data Science not only focuses on finding solutions but also predicts the future with past patterns or insights.


A data analyst’s job is to analyze data in order to make decisions.
A data scientist’s job is to provide insightful data visualizations from raw data that are easily understandable.




Become an expert in Data Scientist. Enroll now in PG program in Data Science and Machine Learning from MITxMicroMasters"
 How is Python Useful?,"Python is widely recognized as an exceptionally advantageous programming language due to its versatility and simplicity. Its extensive range of applications and associated benefits have established it as a preferred choice among developers. Notably, Python stands out in terms of readability and user-friendliness.
Its syntax is meticulously designed to be intuitive and concise, enabling ease in coding, comprehension, and maintenance. Additionally, Python offers a comprehensive standard library that encompasses a diverse collection of pre-built modules and functions. This wealth of resources substantially minimizes the time and effort expended by developers, streamlining the execution of routine programming tasks."
 How R is Useful in the Data Science Domain?,"Here are some ways in which R is useful in the data science domain:

Data Manipulation and Analysis: R offers a comprehensive collection of libraries and functions that facilitate proficient data manipulation, transformation, and statistical analysis.
Statistical Modeling and Machine Learning: R offers a wide range of packages for advanced statistical modeling and machine learning tasks, empowering data scientists to build predictive models and perform complex analyses.
Data Visualization: R’s extensive visualization libraries enable the creation of visually appealing and insightful plots, charts, and graphs.
Reproducible Research: R supports the integration of code, data, and documentation, facilitating reproducible workflows and ensuring transparency in data science projects."
 What is Supervised Learning?,"Supervised learning is a machine learning approach in which an algorithm learns from labeled training data to make predictions or classify new, unseen data. It involves the use of input data and corresponding output labels, allowing the algorithm to learn patterns and relationships. The goal is to generalize the learned patterns and accurately predict outputs for new input data based on the learned patterns."
 What is Unsupervised Learning?,"Unsupervised learning is a machine learning approach wherein an algorithm uncovers patterns and structures within unlabeled data, operating without explicit guidance or predetermined output labels. Its objective is to reveal hidden relationships, patterns, and clusters present in the data. Unlike supervised learning, the algorithm autonomously explores the data to identify inherent structures and draw inferences, proving valuable for exploratory data analysis and the discovery of novel insights.
Get 100% Hike!Master Most in Demand Skills Now !






































  By providing your contact details, you agree to our Terms of Use & Privacy Policy"
 What do you understand about Linear Regression?,"Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In linear regression, we try to understand how the dependent variable changes with respect to  the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression."
 What do you understand by logistic regression?,"Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.

Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve.

So, basically in logistic regression, the Y value lies within the range of 0 and 1. This is how logistic regression works."
 What is a confusion matrix?,"The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.

True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works."
 What do you understand about the true-positive rate and false-positive rate?,"True positive rate: In Machine Learning, true-positive rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly identified. Formula: 
True Positive Rate = True Positives/Positives
False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false-positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: 
False-Positive Rate = False-Positives/Negatives.
Check out this comprehensive Data Science Course in India!"
 How is Data Science different from traditional application programming?,"Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.
In traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially, for data that even computers had a hard time understanding, e.g., images, videos, etc.
Data Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use data science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs.
This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of black box, and we cannot understand how the inputs are being transformed into outputs.
However, If the accuracy is good enough, then we can use the system (also called a model).
As described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.
Interested to learn Data Science skills? Check our Data Science course in Kottayam Now!"
 Explain the differences between supervised and unsupervised learning.,"Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.




Supervised Learning
Unsupervised Learning


Works on the data that contains both inputs and the expected output, i.e., the labeled data
Works on the data that contains no mappings from input to output, i.e., the unlabeled data


Used to create models that can be employed to predict or classify things
Used to extract meaningful information out of large volumes of data


Commonly used supervised learning algorithms: Linear regression, decision tree, etc.
Commonly used unsupervised learning algorithms: K-means clustering, Apriori algorithm, etc.





Career Transition"
 What is the difference between long format data and wide format data?,"Long Format Data
Wide Format Data


A long format data has a column for possible variable types and a column for the values of those variables.
Whereas, Wide data has a column for each variable.


Each row in the long format represents a one-time point per subject. As a result, each topic will contain many rows of data.
The repeated responses of a subject will be in a single row, with each response in its own column, in the wide format.


This data format is most typically used in R analysis and for writing to log files at the end of each experiment.
This data format is most widely used in data manipulations, and stats programmes for repeated measures ANOVAs and is seldom used in R analysis.


A long format contains values that do repeat in the first column.
A wide format contains values that do not repeat in the first column.


Use df.melt() to convert the wide form to long form
use df.pivot().reset_index() to convert the long form into wide form



If you are a UI/UX design enthusiast, learn how to become a UI UX designer with a step-by-step guide."
 Mention some techniques used for sampling. What is the main advantage of sampling?,"Sampling is defined as the process of selecting a sample from a group of people or from any particular kind for research purposes. It is one of the most important factors which decides the accuracy of a research/survey result.
Mainly, there are two types of sampling techniques:
Probability sampling: It involves random selection which makes every element get a chance to be selected. Probability sampling has various subtypes in it, as mentioned below:

Simple Random Sampling
Stratified sampling
Systematic sampling
Cluster Sampling
Multi-stage Sampling

Non- Probability Sampling: Non-probability sampling follows non-random selection which means the selection is done based on your ease or any other required criteria. This helps to collect the data easily. The following are various types of sampling in it:

Convenience Sampling
Purposive Sampling
Quota Sampling
Referral /Snowball Sampling"
 What is bias in data science?,"Bias is a type of error that occurs in a data science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc."
 What is dimensionality reduction?,"Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information."
 Why is Python used for data cleaning in DS?,"Data Scientists have to clean and transform huge data sets into a form that they can work with. It is important to deal with redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.
Python libraries such as  Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for data cleaning and analysis. These libraries are used to load and clean the data and do effective analysis. For instance, you might decide to remove outliers that are beyond a certain standard deviation from the mean of a numerical column.

mean = df[""Price""].mean();

std = df[""Price""].std();

threshold = mean + (3 * std);  # Set a threshold for outliers

df = df[df[""Price""] &lt; threshold]  # Remove outliers

Hence, this is how the process of data cleaning is done using python libraries in the field of data science.
Learn more about Data Cleaning in a Data Science Tutorial!"
 Why is R used in Data Visualization?,"R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.
It has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets."
 What are the popular libraries used in Data Science?,"Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:

TensorFlow: Supports parallel computing with impeccable library management backed by Google.
SciPy: Mainly used for solving differential equations, multidimensional programming, data manipulation, and visualization through graphs and charts.
Pandas: Used to implement the ETL(Extracting, Transforming, and Loading the datasets) capabilities in business applications.
Matplotlib: Being free and open-source, it can be used as a replacement for MATLAB, which results in better performance and low memory consumption.
PyTorch: Best for projects which involve ,machine learning algorithms and deep neural networks.

Interested to learn more about Data Science, check out our Data Science Course in New York!

Courses you may like"
 What are important functions used in Data Science?,"Within the realm of data science, various pivotal functions assume critical roles across diverse tasks. Among these, two foundational functions are the cost function and the loss function.
Cost function: Also referred to as the objective function, the cost function holds substantial utility within machine learning algorithms, especially in optimization scenarios. Its purpose is to quantify the disparity between predicted values and actual values. Minimizing the cost function entails optimizing the model’s parameters or coefficients, aiming to achieve an optimal solution.
Loss function: Loss functions possess significant significance in supervised learning endeavors. They evaluate the discrepancy or error between predicted values and actual labels. The selection of a specific loss function depends on the problem at hand, such as employing mean squared error (MSE) for regression tasks or cross-entropy loss for classification tasks. The loss function guides the model’s optimization process during training, ultimately bolstering accuracy and overall performance."
 What is k-fold cross-validation?,"In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes."
 Explain how a recommender system works.,"A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.
For example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with similar tastes like watching."
 What is Poisson Distribution?,"The Poisson distribution is a statistical probability distribution used to represent the occurrence of events within a specific interval of time or space. It is commonly employed to characterize infrequent events that happen independently and at a consistent average rate, such as quantifying the number of incoming phone calls received within a given hour.
Learn how to make sure people type in the right email on your website. It’s easy with JavaScript – read email validation in JavaScript"
 What is a normal distribution?,"Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or the right, or it could all be jumbled up.
Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution."
 What is Deep Learning?,"Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them.
Deep Learning is an advanced version of neural networks to make the machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer."
 What is CNN (Convolutional Neural Network)?,"A Convolutional Neural Network (CNN) is an advanced deep learning architecture designed specifically for analyzing visual data, such as images and videos. It is composed of interconnected layers of neurons that utilize convolutional operations to extract meaningful features from the input data. CNNs exhibit remarkable effectiveness in tasks like image classification, object detection, and image recognition, thanks to their inherent ability to autonomously learn hierarchical representations and capture spatial relationships within the data, eliminating the need for explicit feature engineering."
 What is an RNN (recurrent neural network)?,"A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results."
 Explain selection bias.,"Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study."
" Between Python and R, which one will you choose for analyzing the text, and why?","Due to the following factors, Python will outperform R for text analytics:

Python’s Pandas module provides high-performance data analysis capabilities as well as simple-to-use data structures.
Python does all sorts of text analytics more quickly."
 Explain the purpose of data cleaning,"Data cleaning’s primary goal is to rectify or eliminate inaccurate, corrupted, improperly formatted, duplicate, or incomplete data from a dataset. This often yields better outcomes and a higher return on investment for marketing and communications efforts."
 What do you understand from Recommender System? and State its application,"Recommender Systems are a subclass of information filtering systems designed to forecast the preferences or ratings given to a product by a user.
The Amazon product suggestions page is an example of a recommender system in use. Based on the user’s search history and previous orders, this area contains products."
 What is Gradient Descent?,"An iterative first-order optimization process called gradient descent (GD) is used to locate the local minimum and maximum of a given function. This technique is frequently applied in machine learning (ML) and deep learning (DL) to minimize a cost/loss function (for example, in linear regression)."
 What are the various skills required to become Data Scientist?,"The following abilities are necessary to become a certified Data Scientist:

Having familiarity with built-in data types like lists, tuples, sets, and related.
N-dimensional NumPy array knowledge is required.
Being able to use Pandas and Dataframes.
Strong holdover performance in vectors with only one element.
Hands-on experience with Tableau and PowerBI."
 What is TensorFlow?,"A free and open-source software library for machine learning and artificial intelligence is called TensorFlow. It enables programmers to build dataflow graphs, which are representations of the flow of data among processing nodes in a graph."
 What is Dropout?,"In Data Science, the term “dropout” refers to the process of randomly removing visible and hidden network units. By eliminating up to 20% of the nodes, they avoid overfitting the data and allow for the necessary space to be set up for the network’s iterative convergence process."
 State any five Deep Learning Frameworks.,"Some of the Deep Learning frameworks are:

Caffe
Keras
TensorFlow
Pytorch
Chainer
Microsoft Cognitive Toolkit"
 Define Neural Networks and its types,"Neural Networks are computational models that derive their principles from the structure and functionality of the human brain. Consisting of interconnected artificial neurons organized in layers, Neural Networks exhibit remarkable capacities in learning and discerning patterns within datasets. Consequently, they assume a pivotal role in diverse domains including pattern recognition, classification, and optimization, thereby providing invaluable solutions in the realm of artificial intelligence.
There exist various types of Neural Networks, including:

Feedforward Neural Networks: These networks facilitate a unidirectional information flow, progressing from input to output. They find frequent application in tasks involving pattern recognition and classification.
Convolutional Neural Networks (CNNs): Specifically tailored for grid-like data, such as images or videos, CNNs leverage convolutional layers to extract meaningful features. Their prowess lies in tasks like image classification and object detection.
Recurrent Neural Networks (RNNs): RNNs are particularly adept at handling sequential data, wherein the present output is influenced by past inputs. They are extensively utilized in domains such as language modeling and time series analysis.
Long Short-Term Memory (LSTM) Networks: This variation of RNNs addresses the issue of vanishing gradients and excels at capturing long-term dependencies in data. LSTM networks find wide-ranging applications in areas like speech recognition and natural language processing.
Generative Adversarial Networks (GANs): GANs consist of a generator and a discriminator that is trained in a competitive manner. They are employed to generate new data samples and are helpful for tasks like image generation and text synthesis.

These examples represent only a fraction of the available variations and architectures tailored to specific data types and problem domains.

Data Science Interview Questions For Intermediate"
 What is the ROC curve?,"It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph:"
 What do you understand by a decision tree?,"A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.

Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which give the final decision according to the condition.
Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!"
 What do you understand by a random forest model?,"It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model."
" Two candidates, Aman and Mohan appear for a Data Science Job interview. The probability of Aman cracking the interview is 1/8 and that of Mohan is 5/12. What is the probability that at least one of them will crack the interview?","The probability of Aman getting selected for the interview is 1/8
P(A) = 1/8
The probability of Mohan getting selected for the interview is 5/12
P(B)=5/12
Now, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means
P(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)
Where P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.
To calculate the final answer, we first have to find out the value of P(A ∩ B)
So, P(A ∩ B) = P(A) * P(B)
1/8 * 5/12
5/96
Now, put the value of P(A ∩ B) into equation (1)
P(A U B) =P(A)+ P(B) – (P(A ∩ B))
1/8 + 5/12 -5/96
So, the answer will be 47/96."
 How is Data modeling different from Database design?,"Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques.
Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters."
 What is precision?,"Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:"
 What is a recall?,Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:
 What is the F1 score and how to calculate it?,"F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score:"
 What is a p-value?,"P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis."
 Why do we use p-value?,We use the p-value to understand whether the given data really describes the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ is true:
 What is the difference between an error and a residual error?,"An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error."
 Why do we use the summary function?,"The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:

Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better."
 How are Data Science and Machine Learning related to each other?,"Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.
Data Science is a broad field that deals with large volumes of data and allows us to draw insights from this voluminous data. The entire process of data science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.
Machine Learning, on the other hand, can be thought of as a sub-field of data science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.
In short, data science deals with gathering data, processing it, and finally, drawing insights from it. The field of data science that deals with building models using algorithms is called machine learning. Therefore, machine learning is an integral part of data science."
" Explain univariate, bivariate, and multivariate analyses.","When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.

Univariate analysis: Univariate analysis involves analyzing data with only one variable or, in other words, a single column or a vector of the data. This analysis allows us to understand the data and extract patterns and trends from it. Example: Analyzing the weight of a group of people.
Bivariate analysis: Bivariate analysis involves analyzing the data with exactly two variables or, in other words, the data can be put into a two-column table. This kind of analysis allows us to figure out the relationship between the variables. Example: Analyzing the data that contains temperature and altitude.
Multivariate analysis: Multivariate analysis involves analyzing the data with more than two variables. The number of columns of the data can be anything more than two. This kind of analysis allows us to figure out the effects of all other variables (input variables) on a single variable (the output variable).

Example: Analyzing data about house prices, which contains information about the houses, such as locality, crime rate, area, the number of floors, etc."
 How can we handle missing data?,"To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation.
For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.
One way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contains these values.
Another way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.
Finally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem anyway."
 What is the benefit of dimensionality reduction?,"Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data.
The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy."
 What is a bias-variance trade-off in Data Science?,"When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.
Bias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making the model more complex can lead to reducing bias, if we make the model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, the bias reduces and the variance increases, and if we reduce complexity, the bias increases and the variance reduces. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance."
 What is RMSE?,"RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:
First, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors.
After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate."
 What is a kernel function in SVM?,"In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable."
 How can we select an appropriate value of k in k-means?,"Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.
This is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm."
 How can we deal with outliers?,"Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.
In case the outliers are not that extreme, then we can try:

A different kind of model. For example, if we were using a linear model, then we can choose a non-linear model
Normalizing the data, which will shift the extreme values closer to other data points
Using algorithms that are not so affected by outliers, such as random forest, etc."
 How to calculate the accuracy of a binary classification algorithm using its confusion matrix?,"In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:

True positives: Number of observations correctly classified as True
True negatives: Number of observations correctly classified as False
False positives: Number of observations incorrectly classified as True
False negatives: Number of observations incorrectly classified as False

To calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations."
 What is ensemble learning?,"When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy.
However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning."
 Explain collaborative filtering in recommender systems.,"Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc.
If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A.
In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not."
 Explain content-based filtering in recommender systems.,"Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in.
For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well.
In other words, here, the content of the movie is taken into consideration when generating recommendations for users."
 Explain bagging in Data Science.,"Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model.
Once all the models are trained, then it’s time to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that have the highest frequency."
 Explain boosting in data science.,"Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it.
In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well."
 Explain stacking in data science.,"Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners.
However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models."
 Explain how machine learning is different from deep learning.,"A field of computer science, machine learning is a subfield of data science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.
Deep Learning, on the other hand, is a field in machine learning that deals with building machine learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In deep learning, we make heavy use of deeply connected neural networks with many layers."
 What does the word ‘Naive’ mean in Naive Bayes?,"Naive Bayes is a data science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.
It has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.
To learn more about Data Science, check out our Data Science Course in Hyderabad."
 What is batch normalization?,"One method for attempting to enhance the functionality and stability of the neural network is batch normalization. To do this, normalize the inputs in each layer such that the mean output activation stays at 0 and the standard deviation is set to 1."
 What do you understand from cluster sampling and systematic sampling?,"Cluster sampling is also known as the probability sampling approach where you can divide a population into groups, such as districts or schools, and then select a representative sample from among these groups at random. A modest representation of the population as a whole should be present in each cluster.
A probability sampling strategy called systematic sampling involves picking people from the population at regular intervals, such as every 15th person on a population list. The population can be organized randomly to mimic the benefits of simple random sampling."
 What is the Computational Graph?,"A directed graph with variables or operations as nodes is a computational graph. Variables can contribute to operations with their value, and operations can contribute their output to other operations. In this manner, each node in the graph establishes a function of the variables."
 What is the difference between Batch and Stochastic Gradient Descent?,"The differences between Batch and Stochastic Gradient Descent are as follows:




Batch 
Stochastic Gradient Descent


Provides assistance in calculating the gradient utilizing the entire set of data.
Helps in calculating the gradient using only a single sample.


Takes time to converge.
Takes less time to converge.


The volume is substantial enough for analysis.
The volume is lower for analysis purposes.


Updates the weight infrequently.
Updates the weight more frequently."
 What is an activation function?,"An activation function is a function that is incorporated into an artificial neural network to aid in the network’s learning of complicated patterns in the input data. In contrast to a neuron-based model seen in human brains, the activation function determines what signals should be sent to the following neuron at the very end."
 How Do You Build a random forest model?,"The steps for creating a random forest model are as follows: 

Choose n from a dataset of k records. 
Create distinct decision trees for each of the n data values being taken into account. From each of them, a projected result is obtained.  
Each of the findings is subjected to a voting mechanism.  
The final outcome is determined by whose prediction received the most support."
" Can you avoid overfitting your model? if yes, then how?","In actuality, data models may be overfitting. For it, the strategies listed below can be applied:

Increase the amount of data in the dataset under study to make it simpler to separate the links between the input and output variables. 
To discover important traits or parameters that need to be examined, use feature selection. 
Use regularization strategies to lessen the variation of the outcomes a data model generates. 
Rarely, datasets are stabilized by adding a little amount of noisy data. This practice is called data augmentation."
 What is Cross Validation?,"Cross-validation is a model validation method used to assess the generalizability of statistical analysis results to other data sets. It is frequently applied when forecasting is the main objective and one wants to gauge how well a model will work in real-world applications.
In order to prevent overfitting and gather knowledge on how the model will generalize to different data sets, cross-validation aims to establish a data set to test the model during the training phase (i.e. validation data set)."
 What is variance in Data Science?,"Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting."
 What is pruning in a decision tree algorithm?,"Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed."
 What is entropy in a decision tree algorithm?,"In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.
Entropy(D) = - p * log2(p) - (1 - p) * log2(1 - p)
where: 
Entropy(D) represents the entropy of the dataset D
p represents the proportion of positive class instances in D
log2 represents the logarithm to the base 2.
For example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.
Additionally, In a decision tree algorithm, multi-class entropy is a measure used to evaluate the impurity or disorder of a dataset with respect to the class labels when there are multiple classes involved. It is commonly used as a criterion to make decisions about splitting nodes in a decision tree."
 What information is gained in a decision tree algorithm?,"When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data.
Let’s consider a practical example to gain a better understanding of how information gain operates within a decision tree algorithm. Imagine we have a dataset containing customer information such as age, income, and purchase history. Our objective is to predict whether a customer will make a purchase or not.
To determine which attribute provides the most valuable information, we calculate the information gain for each attribute. If splitting the data based on income leads to subsets with significantly reduced entropy, it indicates that income plays a crucial role in predicting purchase behavior. Consequently, income becomes a crucial factor in constructing the decision tree as it offers valuable insights.
By maximizing information gain, the decision tree algorithm identifies attributes that effectively reduce uncertainty and enable accurate splits. This process enhances the model’s predictive accuracy, enabling informed decisions pertaining to customer purchases.
Explore this Data Science Course in Delhi and master decision tree algorithm.

Data Science Interview Questions For Experienced"
" From the below given ‘diamonds’ dataset, extract only those rows where the ‘price’ value is greater than 1000 and the ‘cut’ is ideal.","First, we will load the ggplot2 package:
library(ggplot2)
Next, we will use the dplyr package:
library(dplyr)// It is based on the grammar of data manipulation.
To extract those particular records, use the below command:
diamonds %>% filter(price>1000 & cut==”Ideal”)-> diamonds_1000_idea"
" Make a scatter plot between ‘price’ and ‘carat’ using ggplot. ‘Price’ should be on the y-axis, ’carat’ should be on the x-axis, and the ‘color’ of the points should be determined by ‘cut.’","We will implement the scatter plot using ggplot.
The ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.
So, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.
Code:
>ggplot(data=diamonds, aes(x=caret, y=price, col=cut))+geom_point()"
 Introduce 25 percent missing values in this ‘iris’ dataset and impute the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median.’,"To introduce missing values, we will be using the missForest package:

library(missForest)
Using the prodNA function, we will be introducing 25 percent of missing values:

Iris.mis&lt;-prodNA(iris,noNA=0.25)

For inputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:

library(Hmisc)
iris.mis$Sepal.Length&lt;-with(iris.mis, impute(Sepal.Length,mean))
iris.mis$Petal.Length&lt;-with(iris.mis, impute(Petal.Length,median))"
" Implement simple linear regression in R on this ‘mtcars’ dataset, where the dependent variable is ‘mpg’ and the independent variable is ‘disp.’","Here, we need to find how ‘mpg’ varies w.r.t displacement of the column.
We need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.
So, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.
Therefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.
Here, we will use the following code:
library(caret)

split_tag<-createDataPartition(mtcars$mpg, p=0.65, list=F)

mtcars&#91;split_tag,&#93;->train

mtcars[-split_tag,]->test

lm(mpg-data,data=train)->mod_mtcars

predict(mod_mtcars,newdata=test)->pred_mtcars

>head(pred_mtcars)
Explanation:
Parameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).
Second is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in a split_tag object.
Once we have the split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.
Similarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.
So, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.
We will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.
lm(mpg-data,data=train)->mod_mtcars

Now, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and the second is the dataframe on which we have to predict values.
Thus, we have to predict values for the test set and then store them in pred_mtcars.
predict(mod_mtcars,newdata=test)->pred_mtcars

Output:
These are the predicted values of mpg for all of these cars.

So, this is how we can build a simple linear model on top of this mtcars dataset."
 Calculate the RMSE values for the model building.,"When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used.
Code:
cbind(Actual=test$mpg, predicted=pred_mtcars)->final_data

as.data.frame(final_data)->final_data

error<-(final_data$Actual-final_data$Prediction)

cbind(final_data,error)->final_data

sqrt(mean(final_data$error)^2)
Explanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:

cbind(Actual=test$mpg, predicted=pred_mtcars)->final_data

Our actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values, and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:
as.data.frame(final_data)->final_data

We will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:

error &lt;-(final_data$Actual-final_data$Prediction)

Then, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:

cbind(final_data,error)->final_data //binding error object to this final_data

Here, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:
Sqrt(mean(final_data$error)^2)
Output:
[1] 4.334423
Note: Lower the value of RMSE, the better the model. R and Python are two of the most important programming languages for Machine Learning Algorithms."
 Implement simple linear regression in Python on this ‘Boston’ dataset where the dependent variable is ‘medv’ and the independent variable is ‘lstat.’,"Simple Linear Regression
import pandas as pd

data=pd.read_csv(‘Boston.csv’)     //loading the Boston dataset

data.head()  //having a glance at the head of this data

data.shape


Let us take out the dependent and the independent variables from the dataset:
data1=data.loc[:,[‘lstat’,’medv’]]

data1.head()
Visualizing Variables
import matplotlib.pyplot as plt

data1.plot(x=’lstat’,y=’medv’,style=’o’)

plt.xlabel(‘lstat’)

plt.ylabel(‘medv’)

plt.show()
Here, ‘medv’ is basically the median value of the price of the houses, and we are trying to find out the median values of the price of the houses with respect to to the lstat column.
We will separate the dependent and the independent variable from this entire dataframe:
data1=data.loc[:,[‘lstat’,’medv’]]
The only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.
Now, we would also do a visualization w.r.t to these two columns:
import matplotlib.pyplot as plt

data1.plot(x=’lstat’,y=’medv’,style=’o’)

plt.xlabel(‘lstat’)

plt.ylabel(‘medv’)

plt.show()
Preparing the Data
X=pd.Dataframe(data1[‘lstat’])

Y=pd.Dataframe(data1[‘medv’])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)

from sklearn.linear_model import LinearRegression

regressor=LinearRegression()

regressor.fit(X_train,y_train)
 
print(regressor.intercept_)
Output :
34.12654201

print(regressor.coef_)//this is the slope
Output :
[[-0.913293]]
By now, we have built the model. Now, we have to predict the values on top of the test set:
y_pred=regressor.predict(X_test)//using the instance and the predict function and pass the X_test object inside the function and store this in the y_pred object

Now, let’s have a glance at the rows and columns of the actual values and the predicted values:
Y_pred.shape, y_test.shape
Output :
((102,1),(102,1))
Further, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.
from sklearn import metrics import NumPy as np

print(‘Mean Absolute Error: ’, metrics.mean_absolute_error(y_test, y_pred))

print(‘Mean Squared Error: ’, metrics.mean_squared_error(y_test, y_pred))

print(‘Root Mean Squared Error: ’, np.sqrt(metrics.mean_absolute_error(y_test, y_pred))

Output:
Mean Absolute Error: 4.692198

Mean Squared Error: 43.9198

Root Mean Squared Error: 6.6270"
 Implement logistic regression on this ‘heart’ dataset in R where the dependent variable is ‘target’ and the independent variable is ‘age.’,"For loading the dataset, we will use the read.csv function:

read.csv(“D:/heart.csv”)->heart
str(heart)

In the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.
Thus, we will use the as.factor function and convert these integer values into categorical data.
We will pass on heart$target column over here and store the result in heart$target as follows:
as.factor(heart$target)->;heart$target
Now, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.
To build a logistic regression model, we will use the glm function:
glm(target~age, data=heart, family=”binomial”)->;log_mod1
Here, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.
family=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.
We will have a glance at the summary of the model that we have just built:
summary(log_mod1)

We can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.
Now, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.
This null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.
Residual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.
This basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.
As we have built the model, it’s time to predict some values:
predict(log_mod1, data.frame(age=30), type=”response”)

predict(log_mod1, data.frame(age=50), type=”response”)

predict(log_mod1, data.frame(age=29:77), type=”response”)


Now, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:

&gt;library(caret)

Split_tag&lt;- createDataPartition(heart$target, p=0.70, list=F) heart[split_tag,]-&gt;train

heart[-split_tag,]-&gt;test

glm(target~age, data=train,family=”binomial”)-&gt;log_mod2

predict(log_mod2, newdata=test, type=”response”)-&gt;pred_heart

range(pred_heart)"
 Build a ROC curve for the model built,"The below code will help us in building the ROC curve:
library(ROCR)

prediction(pred_heart, test$target)-> roc_pred_heart

performance(roc_pred_heart, “tpr”, “fpr”)->roc_curve

plot(roc_curve, colorize=T)
Graph: 
Go through this Data Science Course in Enakulam to get a clear understanding of Data Science!"
" Build a confusion matrix for the model where the threshold value for the probability of predicted values is 0.6, and also find the accuracy of the model.","Accuracy is calculated as:
Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)
To build a confusion matrix in R, we will use the table function:
table(test$target,pred_heart > 0.6)
Here, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.
Then, we calculate the accuracy by the formula for calculating Accuracy."
 Build a logistic regression model on the ‘customer_churn’ dataset in Python. The dependent variable is ‘Churn’ and the independent variable is ‘MonthlyCharges.’ Find the log_loss of the model.,"First, we will load the pandas dataframe and the customer_churn.csv file:
customer_churn=pd.read_csv(“customer_churn.csv”)

After loading this dataset, we can have a glance at the head of the dataset by using the following command:
customer_churn.head()
Now, we will separate the dependent and the independent variables into two separate objects:
x=pd.Dataframe(customer_churn[‘MonthlyCharges’])

y=customer_churn[‘ Churn’]

#Splitting the data into training and testing sets

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3, random_state=0)
Now, we will see how to build the model and calculate log_loss.
from sklearn.linear_model, we have to import LogisticRegression

l=LogisticRegression()

l.fit(x_train,y_train)

y_pred=l.predict_proba(x_test)
As we are supposed to calculate the log_loss, we will import it from sklearn.metrics:
from sklearn.metrics import log_loss

print(log_loss(y_test,y_pred)//actual values are in y_test and predicted are in y_pred
Output:
0.5555020595194167
Become a master of Data Science by going through this online Data Science Course in Dehradun!"
" Build a decision tree model on ‘Iris’ dataset where the dependent variable is ‘Species,’ and all other columns are independent variables. Find the accuracy of the model built.","To build a decision tree model, we will be loading the party package:
#party package

library(party)

#splitting the data

library(caret)

split_tag<-createDataPartition(iris$Species, p=0.65, list=F)

iris&#91;split_tag,&#93;->train

iris[~split_tag,]->test

#building model

mytree<-ctree(Species~.,train)&#91;/code&#93;

Now we will plot the model

&#91;code language=""javascript""&#93;plot(mytree)&#91;/code&#93;

<strong>Model:</strong>
<img class=""alignnone size-full wp-image-204079"" src=""https://intellipaat.com/blog/wp-content/uploads/2015/09/Graphics-06.jpg"" alt=""Data Science Interview Questions and Answers - Intellipaat"" width=""800"" height=""380"" />

[code language=""javascript""]#predicting the values

predict(mytree,test,type=’response’)->mypred
After this, we will predict the confusion matrix and then calculate the accuracy using the table function:
table(test$Species, mypred)"
" Build a random forest model on top of this ‘CTG’ dataset, where ‘NSP’ is the dependent variable and all other columns are independent variables.","We will load the CTG dataset by using read.csv:
data<read.csv(“C:/Users/intellipaat/Downloads/CTG.csv”,header=True)

str(data)&#91;/code&#93;

Converting the integer type to a factor

&#91;code language=""javascript""&#93;data$NSP<-as.factor(data$NSP)

table(data$NSP)

#data partition

set.seed(123)

split_tag<-createDataPartition(data$NSP, p=0.65, list=F)

data&#91;split_tag,&#93;->train

data[~split_tag,]->test

#random forest -1

library(randomForest)

set.seed(222)

rf-<-randomForest(NSP~.,data=train)

rf

#prediction

predict(rf,test)->p1
Building confusion matrix and calculating accuracy:
table(test$NSP,p1)

If you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!"
 Write a function to calculate the Euclidean distance between two points.,"The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:
√(((x1 - x2) ^ 2) + ((y1 - y2) ^ 2))
Code for calculating the Euclidean distance is as given below:
def euclidean_distance(P1, P2):
return (((P1[0] - P2[0]) ** 2) + ((P1[1] - P2[1]) ** 2)) ** .5"
 Write code to calculate the root mean square error (RMSE) given the lists of values as actual and predicted.,"To calculate the root mean square error (RMSE), we have to:

 Calculate the errors, i.e., the differences between the actual and the predicted values
 Square each of these errors
 Calculate the mean of these squared errors
 Return the square root of the mean

The code in Python for calculating RMSE is given below:

def rmse(actual, predicted):
errors = [abs(actual[i] - predicted[i]) for i in range(0, len(actual))]
squared_errors = [x ** 2 for x in errors]
mean = sum(squared_errors) / len(squared_errors)
return mean ** .5

Check out this Machine Learning Course to get an in-depth understanding of Machine Learning."
 Mention the different kernel functions that can be used in SVM.,"In SVM, there are four types of kernel functions:
 

Linear kernel
In SVM (Support Vector Machines), a linear kernel is a type of kernel function used to transform input data into a higher-dimensional feature space. It is represented by the equation K(x, y) = x • y, where x and y are feature vectors. The linear kernel calculates the dot product between the two vectors to measure their similarity or dissimilarity.
Polynomial kernel
A polynomial kernel is a type of kernel function used to transform input data into a higher-dimensional feature space. It is represented by the equation K(x, y) = (x • y + c)^d, where x and y are feature vectors, c is a constant, and d is the degree of the polynomial. The polynomial kernel captures nonlinear relationships between data points by raising the dot product to a specified power.
Radial basis kernel
In SVM (Support Vector Machines), a radial basis kernel, also known as the Gaussian kernel, is a popular kernel function used for non-linear classification. It is represented by the equation K(x, y) = exp(-gamma * ||x – y||^2), where x and y are feature vectors, and gamma is a parameter that determines the influence of each training example. The radial basis kernel measures the similarity between data points based on their Euclidean distance in the feature space.
Sigmoid kernel
The sigmoid kernel is a type of non-linear kernel function commonly employed for classification tasks. It can be mathematically described by the equation K(x, y) = tanh(alpha * x * y + c), where x and y represent feature vectors, and alpha and c are parameters determining the sigmoid function’s shape. By utilizing the sigmoid kernel, Support Vector Machines (SVMs) can project data onto a higher-dimensional space, enabling the creation of non-linear decision boundaries for accurate classification."
 How to detect if the time series data is stationary?,"Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary."
 Write code to calculate the accuracy of a binary classification algorithm using its confusion matrix.,"We can use the code given below to calculate the accuracy of a binary classification algorithm:

def accuracy_score(matrix):
true_positives = matrix[0][0]
true_negatives = matrix[1][1]
total_observations = sum(matrix[0]) + sum(matrix[1])
return (true_positives + true_negatives) / total_observations"
 What does root cause analysis mean?,"Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas."
 What is A/B testing?,"A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B.
The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product.
If the rating of product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.
Check out this Python Course to get deeper into Python programming."
" Out of collaborative filtering and content-based filtering, which one is considered better, and why?","Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations.
However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future.
For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.
In the case of content-based filtering, we make use of users’ own likes and dislikes which are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users."
" In the following confusion matrix, calculate precision and recall.","Total = 510
Actual


Predicted

P
N


P
156
11


N
16
327




The formulae for precision and recall are given below.
Precision:
(True Positive) / (True Positive + False Positive)
Recall:
(True Positive) / (True Positive + False Negative)
Based on the given data, precision and recall are:
Precision: 156 / (156 + 11) = 93.4
Recall: 156 / (156 + 16) = 90.7"
 Write a function that when called with a confusion matrix for a binary classification model returns a dictionary with its precision and recall.,"We can use the below for this purpose:
def calculate_precsion_and_recall(matrix):
true_positive  = matrix[0][0]
false_positive  = matrix[0][1]
false_negative = matrix[1][0]
return {
   'precision': (true_positive) / (true_positive + false_positive),
   'recall': (true_positive) / (true_positive + false_negative)"
 What is reinforcement learning?,"Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most cumulative rewards.
A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it.
Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal."
 Explain TF/IDF vectorization.,The expression ‘TF/IDF’ stands for the Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.
 What are the assumptions required for linear regression?,"There are several assumptions required for linear regression. They are as follows:

The data, which is a sample drawn from a population, used to train the model should be representative of the population.
The relationship between independent variables and the mean of dependent variables is linear.
The variance of the residual is going to be the same for any value of an independent variable. It is also represented as X.
Each observation is independent of all other observations.
For any value of an independent variable, the independent variable is normally distributed."
 What happens when some of the assumptions required for linear regression are violated?,"These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.
Strong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance."
 How to deal with unbalanced binary classification?,"Given below are the following points that will teach you to deal with unbalanced binary classification:

Use other formulas to determine the model’s performance, such as precision/recall, F1 score, etc.
Re-sample the data using strategies such as undersampling (decreasing the sample size of the bigger class), oversampling (raising the sample size of the smaller class using repetition, SMOTE, and other similar strategies), and so on.
K-fold cross-validation is used
Use ensemble learning such that each decision tree only takes into account a portion of the bigger class and the complete sample of the smaller class."
 Which cross-validation method would you use for a batch of time series data?,"Instead of utilizing k-fold cross-validation, you should be aware that a time series is fundamentally organized by chronological order and is not made up of randomly dispersed data. Use approaches like forward-chaining, where you model on previous data and then look at forward-facing data, when dealing with time series data."
 How can time-series data be declared as stationery?,The time series is considered stationary when its essential constituents don’t change over time. These variables might be variance or mean. Static time series exhibit no trends nor seasonal impacts. Data from stationary time series are required for data science models.
 Difference between Point Estimates and Confidence Interval.,"Point Estimates: A specific number known as the point estimate provides an estimate of the population parameter. The Maximum Likelihood estimator and the Method of Moments are two common techniques used to produce Population Parameter Point, estimators.
Confidence Interval: The confidence interval provides a range of values that most likely contain the population parameter. It even reveals the likelihood that the population parameter may be found in that specific period. The likelihood or similarity is represented by the Confidence Coefficient (or Confidence level), which is indicated by 1-alpha. The significance level is indicated by alpha."
" Define the terms KPI, lift, model fitting, robustness, and DOE.","KPI: KPI stands for Key Performance Indicator, which evaluates how successfully a company accomplishes its goals.
Lift: Lift is a performance indicator for the target model compared to a random selection model. Lift measures how well the model predicts in comparison to no model.
Model fitting: This describes how well the proposed model conforms to the available data.
Robustness: This refers to how well the system can manage variations and changes.
DOE: DOE refers to the task design with the goal of describing and explaining information variance under postulated circumstances to reflect variables."
 What are LLMs?,"Large Language Models, abbreviated as LLMs, are sophisticated artificial intelligence models designed to process and generate text that resembles human language based on the input they receive. They employ advanced techniques like deep learning, particularly neural networks, to comprehend and produce language patterns, enabling them to answer questions, engage in conversations, and provide information on a broad array of topics.
LLMs undergo training using extensive sets of textual data from diverse sources, including books, websites, and other text-based materials. Through this training, they acquire the ability to recognize patterns, comprehend context, and generate coherent and contextually appropriate responses.
Notable examples of LLMs, such as ChatGPT based on the GPT-3.5 architecture, have been trained on comprehensive and varied datasets to offer accurate and valuable information across different domains. These models possess natural language understanding capabilities and can undertake various tasks such as language translation, content generation, and text completion.
Their versatility allows them to assist users in diverse inquiries and tasks, making them valuable tools across numerous fields, including education, customer service, content creation, and research."
" What is the difference between Type I Error & Type II Error? Also, Explain the Power of the test?","When we perform hypothesis testing we consider two types of Error, Type I error and Type II error, sometimes we reject the null hypothesis when we should not or choose not to reject the null hypothesis when we should. A Type I Error is committed when we reject the null hypothesis when the null hypothesis is actually true. On the other hand, a Type II error is made when we do not reject the null hypothesis and the null hypothesis is actually false. The probability of a Type I error is denoted by α and the probability of Type II error is denoted by β. For a given sample n, a decrease in α will increase β and vice versa. Both α  and β decrease as n increases. The table given below explains the situation around the Type I error and Type II error: Two correct decisions are possible: not rejecting the null hypothesis when the null hypothesis is true and rejecting the null hypothesis when the null hypothesis is false. Conversely, two incorrect decisions are also possible: Rejecting the null hypothesis when the null hypothesis is true(Type I error), and not rejecting the null hypothesis when the null hypothesis is false (Type II error). Type I error is false positive while Type II error is a false negative. Power of Test: The Power of the test is defined as the probability of rejecting the null hypothesis when the null hypothesis is false. Since β is the probability of a Type II error, the power of the test is defined as 1- β.  In advanced statistics, we compare various types of tests based on their size and power, where the size denotes the actual proportion of rejections when the null is true and the power denotes the actual proportion of rejections when the null is false."
 What do you understand by Over-fitting and Under-fitting?,"Overfitting is observed when there is a small amount of data and a large number of variables, If the model we finish with ends up modelling the noise as well, we call it “overfitting” and if we are not modelling all the information, we call it “underfitting”. Most commonly underfitting is observed when a linear model is fitted to a non-linear data. The hope is that the model that does the best on testing data manages to capture/model all the information but leave out all the noise. Overfitting can be avoided by using cross-validation techniques (like K Folds) and regularisation techniques (like Lasso regression)."
 When do you use the Classification Technique over the Regression Technique?,"Classification problems are mainly used when the output is the categorical variable (Discrete) whereas Regression Techniques are used when the output variable is Continuous variable. In the Regression algorithm, we attempt to estimate the mapping function (f) from input variables (x) to numerical (continuous) output variable (y). For example, Linear regression, Support Vector Machine (SVM) and Regression trees. In the Classification algorithm, we attempt to estimate the mapping function (f) from the input variable (x) to the discrete or categorical output variable (y). For example, Logistic Regression, naïve Bayes, Decision Trees & K nearest neighbours. Both Classifications, as well as Regression techniques, are Supervised Machine Learning Algorithms."
 What is the importance of Data Cleansing?," As the name suggests, data cleansing is a process of removing or updating the information that is incorrect, incomplete, duplicated, irrelevant, or formatted improperly. It is very important to improve the quality of data and hence the accuracy and productivity of the processes and organisation as a whole. Real-world data is often captured in formats which have hygiene issues. There are sometimes errors due to various reasons which make the data inconsistent and sometimes only some features of the data. Hence data cleansing is done to filter the usable data from the raw data, otherwise many systems consuming the data will produce erroneous results."
 Which are the important steps of Data Cleaning?,"Different types of data require different types of cleaning, the most important steps of Data Cleaning are: Data Cleaning is an important step before analysing data, it helps to increase the accuracy of the model. This helps organisations to make an informed decision. Data Scientists usually spends 80% of their time cleaning data."
 How is k-NN different from k-means clustering?," K-nearest neighbours is a classification algorithm, which is a subset of supervised learning. K-means is a clustering algorithm, which is a subset of unsupervised learning. And K-NN is a Classification or Regression Machine Learning Algorithm while K-means is a Clustering Machine Learning Algorithm. K-NN is the number of nearest neighbours used to classify or (predict in case of continuous variable/regression) a test sample, whereas K-means is the number of clusters the algorithm is trying to learn from the data."
 What is p-value?," p-value helps you determine the strengths of your results when you perform a hypothesis test. It is a number between 0 and 1. The claim which is on trial is called the Null Hypothesis. Lower p-values, i.e. ≤ 0.05, means we can reject the Null Hypothesis. A high p-value, i.e. ≥ 0.05, means we can accept the Null Hypothesis. An exact p-value 0.05 indicates that the Hypothesis can go either way. P-value is the measure of the probability of events other than suggested by the null hypothesis. It effectively means the probability of events rarer than the event being suggested by the null hypothesis."
 How is Data Science different from Big Data and Data Analytics?," Data Science utilises algorithms and tools to draw meaningful and commercially useful insights from raw data. It involves tasks like data modelling, data cleansing, analysis, pre-processing etc. Big Data is the enormous set of structured, semi-structured, and unstructured data in its raw form generated through various channels.And finally, Data Analytics provides operational insights into complex business scenarios. It also helps in predicting upcoming opportunities and threats for an organisation to exploit. Essentially, big data is the process of handling large volumes of data. It includes standard practices for data management and processing at a high speed maintaining the consistency of data. Data analytics is associated with gaining meaningful insights from the data through mathematical or non-mathematical processes. Data Science is the art of making intelligent systems so that they learn from data and then make decisions according to past experiences."
 What is the use of Statistics in Data Science?," Statistics in Data Science provides tools and methods to identify patterns and structures in data to provide a deeper insight into it. Serves a great role in data acquisition, exploration, analysis, and validation. It plays a really powerful role in Data Science. Data Science is a derived field which is formed from the overlap of statistics probability and computer science. Whenever one needs to do estimations, statistics is involved. Many algorithms in data science are built on top of statistical formulae and processes. Hence statistics is an important part of data science. Also Read: Practical Ways to Implement Data Science in Marketing"
 What is the difference between Supervised Learning and Unsupervised Learning?," Supervised Machine Learning requires labelled data for training while Unsupervised Machine Learning does not require labelled data. It can be trained on unlabelled data. To elaborate, supervised learning involves training of the model with a target value whereas unsupervised has no known results to learn and it has a state-based or adaptive mechanism to learn by itself. Supervised learning involves high computation costs whereas unsupervised learning has low training cost. Supervised learning finds applications in classification and regression tasks whereas unsupervised learning finds applications in clustering and association rule mining."
 What is a Linear Regression?," The linear regression equation is a one-degree equation with the most basic form being Y = mX + C  where m is the slope of the line and C is the standard error. It is used when the response variable is continuous in nature for example height, weight, and the number of hours. It can be a simple linear regression if it involves continuous dependent variable with one independent variable and a multiple linear regression if it has multiple independent variables. Linear regression is a standard statistical practice to calculate the best fit line passing through the data points when plotted. The best fit line is chosen in such a way so that the distance of each data point is minimum from the line which reduces the overall error of the system. Linear regression assumes that the various features in the data are linearly related to the target. It is often used in predictive analytics for calculating estimates in the foreseeable future."
 What is Logistic Regression?," Logistic regression is a technique in predictive analytics which is used when we are doing predictions on a variable which is dichotomous(binary) in nature.  For example, yes/no or true/false etc. The equation for this method is of the form Y = eX + e – X . It is used for classification based tasks. It finds out probabilities for a data point to belong to a particular class for classification."
 Explain Normal Distribution, Normal Distribution is also called the Gaussian Distribution.  It is a type of probability distribution such that most of the values lie near the mean. It has the following characteristics:
 Mention some drawbacks of the Linear Model, Here a few drawbacks of the linear model:
" Which one would you choose for text analysis, R or Python?"," Python would be a better choice for text analysis as it has the Pandas library to facilitate easy-to-use data structures and high-performance data analysis tools. However, depending on the complexity of data one could use either which suits best."
 What steps do you follow while making a decision tree?, The steps involved in making a decision tree are:
  What is correlation and covariance in statistics?," Correlation is defined as the measure of the relationship between two variables. If two variables are directly proportional to each other, then its positive correlation. If the variables are indirectly proportional to each other, it is known as a negative correlation. Covariance is the measure of how much two random variables vary together."
 What is ‘Naive’ in a Naive Bayes?," A naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, given the class variable. Basically, it’s “naive” because it makes assumptions that may or may not turn out to be correct."
 How can you select k for k-means?, The two methods to calculate the optimal value of k in k-means are: Silhouette score is the most prevalent while determining the optimal value of k.
" What Native Data Structures Can You Name in Python? Of These, Which Are Mutable, and Which Are Immutable?", The native python data structures are: Tuples are immutable. Others are mutable.
 What libraries do data scientists use to plot data in Python?," The libraries used for data plotting are: Apart from these, there are many opensource tools, but the aforementioned are the most used in common practice."
 How is Memory Managed in Python?, Memory management in Python involves a private heap containing all Python objects and data structures. The management of this private heap is ensured internally by the Python memory manager.
 What is a recall?, Recall gives the rate of true positives with respect to the sum of true positives and false negatives. It is also known as true positive rate.
 What are lambda functions?," A lambda function is a small anonymous function. A lambda function can take any number of arguments, but can only have one expression."
 What is reinforcement learning?," Reinforcement learning is an unsupervised learning technique in machine learning. It is a state-based learning technique. The models have predefined rules for state change which enable the system to move from one state to another, while the training phase."
 What is Entropy and Information Gain in decision tree algorithm?," Entropy is used to check the homogeneity of a sample. If the value of entropy is ‘0’ then the sample is completely homogenous. On the other hand, if entropy has a value ‘1’, the sample is equally divided. Entropy controls how a Decision Tree decides to split the data. It actually affects how a Decision Tree draws its boundaries. The information gain depends on the decrease in entropy after the dataset is split on an attribute. Constructing a decision tree is always about finding the attributes that return highest information gain."
 What is Cross-Validation?, It is a model validation technique to asses how the outcomes of a statistical analysis will infer to an independent data set. It is majorly used where prediction is the goal and one needs to estimate the performance accuracy of a predictive model in practice.The goal here is to define a data-set for testing a model in its training phase and limit overfitting and underfitting issues. The validation and the training set is to be drawn from the same distribution to avoid making things worse. Also Read: Why Data Science Jobs Are in Demand
 What is Bias-Variance tradeoff?," The error introduced in your model because of over-simplification of the algorithm is known as Bias. On the other hand, Variance is the error introduced to your model because of the complex nature of machine learning algorithm. In this case, the model also learns noise and perform poorly on the test dataset. The bias-variance tradeoff is the optimum balance between bias and variance in a machine learning model. If you try to decrease bias, the variance will increase and vice-versa. Total Error= Square of bias+variance+irreducible error. Bias variance tradeoff is the process of finding the exact number of features while model creation such that the error is kept minimum, but also taking effective care such that the model does not overfit or underfit."
 Mention the types of biases that occur during sampling?, The three types of biases that occur during sampling are:a. Self-Selection Biasb. Under coverage biasc. Survivorship Bias Self selection is when the participants of the analysis select themselves. Undercoverage occurs when very few samples are selected from a segment of the population. Survivorship bias occurs when the observations recorded at the end of the investigation are a non-random set of those present at the beginning of the investigation.
 What is the Confusion Matrix?," A confusion matrix is a 2X2 table that consists of four outputs provided by the binary classifier. A binary classifier predicts all data instances of a test dataset as either positive or negative. This produces four outcomes- It helps in calculating various measures including error rate (FP+FN)/(P+N), specificity(TN/N), accuracy(TP+TN)/(P+N), sensitivity (TP/P), and precision( TP/(TP+FP) ). A confusion matrix is essentially used to evaluate the performance of a machine learning model when the truth values of the experiments are already known and the target class has more than two categories of data. It helps in visualisation and evaluation of the results of the statistical process."
 Explain selection bias," Selection bias occurs when the research does not have a random selection of participants. It is a distortion of statistical analysis resulting from the method of collecting the sample. Selection bias is also referred to as the selection effect. When professionals fail to take selection bias into account, their conclusions might be inaccurate. Some of the different types of selection biases are:"
 What are exploding gradients?," Exploding Gradients is the problematic scenario where large error gradients accumulate to result in very large updates to the weights of neural network models in the training stage. In an extreme case, the value of weights can overflow and result in NaN values. Hence the model becomes unstable and is unable to learn from the training data."
 Explain the Law of Large Numbers," The ‘Law of Large Numbers’ states that if an experiment is repeated independently a large number of times, the average of the individual results is close to the expected value. It also states that the sample variance and standard deviation also converge towards the expected value."
 What is the importance of A/B testing," The goal of A/B testing is to pick the best variant among two hypotheses, the use cases of this kind of testing could be a web page or application responsiveness, landing page redesign, banner testing, marketing campaign performance etc. The first step is to confirm a conversion goal, and then statistical analysis is used to understand which alternative performs better for the given conversion goal."
 Explain Eigenvectors and Eigenvalues," Eigenvectors depict the direction in which a linear transformation moves and acts by compressing, flipping, or stretching. They are used to understand linear transformations and are generally calculated for a correlation or covariance matrix. The eigenvalue is the strength of the transformation in the direction of the eigenvector. An eigenvector’s direction remains unchanged when a linear transformation is applied to it."
 Why Is Re-sampling Done?, Resampling is done to:
 What is systematic sampling and cluster sampling," Systematic sampling is a type of probability sampling method. The sample members are selected from a larger population with a random starting point but a fixed periodic interval. This interval is known as the sampling interval. The sampling interval is calculated by dividing the population size by the desired sample size. Cluster sampling involves dividing the sample population into separate groups, called clusters. Then, a simple random sample of clusters is selected from the population. Analysis is conducted on data from the sampled clusters."
What are Autoencoders?," An autoencoder is a kind of artificial neural network. It is used to learn efficient data codings in an unsupervised manner. It is utilised for learning a representation (encoding) for a set of data, mostly for dimensionality reduction, by training the network to ignore signal “noise”. Autoencoder also tries to generate a representation as close as possible to its original input from the reduced encoding."
  What are the steps to build a Random Forest Model?,"A Random Forest is essentially a build up of a number of decision trees. The steps to build a random forest model include: Step1: Select ‘k’ features from a total of ‘m’ features, randomly. Here k << m Step2: Calculate node D using the best split point — along the ‘k’ features Step 3: Split the node into daughter nodes using best splitStep 4: Repeat Steps 2 and 3 until the leaf nodes are finalisedStep5: Build a Random forest by repeating steps 1-4 for ‘n’ times to create ‘n’ number of trees."
 How do you avoid the overfitting of your model?,Overfitting basically refers to a model that is set only for a small amount of data. It tends to ignore the bigger picture. Three important methods to avoid overfitting are:
" Differentiate between univariate, bivariate, and multivariate analysis.","Univariate data, as the name suggests, contains only one variable. The univariate analysis describes the data and finds patterns that exist within it. Bivariate data contains two different variables. The bivariate analysis deals with causes, relationships and analysis between those two variables. Multivariate data contains three or more variables. Multivariate analysis is similar to that of a bivariate, however, in a multivariate analysis, there exists more than one dependent variable."
 How is random forest different from decision trees?, A Decision Tree is a single structure. Random forest is a collection of decision trees.
 What is dimensionality reduction? What are its benefits?,"Dimensionality reduction is defined as the process of converting a data set with vast dimensions into data with lesser dimensions — in order to convey similar information concisely. This method is mainly beneficial in compressing data and reducing storage space. It is also useful in reducing computation time due to fewer dimensions. Finally,  it helps remove redundant features — for instance, storing a value in two different units (meters and inches) is avoided. In short, dimensionality reduction is the process of reducing the number of random variables under consideration, by obtaining a set of principal variables. It can be divided into feature selection and feature extraction."
" For the given points, how will you calculate the Euclidean distance in Python? plot1 =  [1,3 ]   ; plot2 =  [2,5]",
 Mention feature selection methods used to select the right variables.,"The methods for feature selection can be broadly classified into two types: Filter Methods: These methods involve: Wrapper Methods: These methods involve Others are Forward Elimination, Backward Elimination for Regression, Cosine Similarity-Based Feature Selection for Clustering tasks, Correlation-based eliminations etc."
 What are the different types of clustering algorithms?," Kmeans Clustering, KNN (K nearest neighbour), Hierarchial clustering, Fuzzy Clustering are some of the common examples of clustering algorithms."
 How should you maintain a deployed model?," A deployed model needs to be retrained after a while so as to improve the performance of the model. Since deployment, a track should be kept of the predictions made by the model and the truth values. Later this can be used to retrain the model with the new data. Also, root cause analysis for wrong predictions should be done."
 Which of the following machine learning algorithms can be used for inputting missing values of both categorical and continuous variables? K-means clustering Linear regression K-NN (k-nearest neighbour) Decision trees, KNN and Kmeans
 What is a ROC Curve? Explain how a ROC Curve works?," AUC – ROC curve is a performance measurement for the classification problem at various thresholds settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s."
 How do you find RMSE and MSE in a linear regression model?, Mean square error is the squared sum of (actual value-predicted value) for all data points. It gives an estimate of the total square sum of errors. Root mean square is the square root of the squared sum of errors.
 Can you cite some examples where a false negative holds more importance than a false positive?, In cases of predictions when we are doing disease prediction based on symptoms for diseases like cancer.
 How can outlier values be treated?," Outlier treatment can be done by replacing the values with mean, mode, or a cap off value. The other method is to remove all rows with outliers if they make up a small proportion of the data. A data transformation can also be done on the outliers."
 How can you calculate accuracy using a confusion matrix?," Accuracy score can be calculated by the formula: (TP+TN)/(TP+TN+FP+FN), where TP= True Positive, TN=True Negatives, FP=False positive, and FN=False Negative."
 What is the difference between “long” and “wide” format data?, Wide-format is where we have a single row for every data point with multiple columns to hold the values of various attributes. The long format is where for each data point we have as many rows as the number of attributes and each row contains the value of a particular attribute for a given data point.
 Explain the SVM machine learning algorithm in detail.," SVM is an ML algorithm which is used for classification and regression. For classification, it finds out a muti dimensional hyperplane to distinguish between classes. SVM uses kernels which are namely linear, polynomial, and rbf. There are few parameters which need to be passed to SVM in order to specify the points to consider while the calculation of the hyperplane."
 What are the various steps involved in an analytics project?, The steps involved in a text analytics project are:
 Explain Star Schema., Star schema is a data warehousing concept in which all schema is connected to a central schema.
 How Regularly Must an Algorithm be Updated?," It completely depends on the accuracy and precision being required at the point of delivery and also on how much new data we have to train on. For a model trained on 10 million rows its important to have new data with the same volume or close to the same volume. Training on 1 million new data points every alternate week, or fortnight won’t add much value in terms of increasing the efficiency of the model."
 What is Collaborative Filtering?, Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users. It works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user.
 How will you define the number of clusters in a clustering algorithm?," By determining the Silhouette score and elbow method, we determine the number of clusters in the algorithm."
 What is Ensemble Learning? Define types.," Ensemble learning is clubbing of multiple weak learners (ml classifiers) and then using aggregation for result prediction. It is observed that even if the classifiers perform poorly individually, they do better when their results are aggregated. An example of ensemble learning is random forest classifier."
 What are the support vectors in SVM?," Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximise the margin of the classifier. Deleting the support vectors will change the position of the hyperplane. These are the points that help us build our SVM."
 What is pruning in Decision Tree?, Pruning is the process of reducing the size of a decision tree. The reason for pruning is that the trees prepared by the base algorithm can be prone to overfitting as they become incredibly large and complex.
 What are the various classification algorithms?," Different types of classification algorithms include logistic regression, SVM, Naive Bayes, decision trees, and random forest."
 What are Recommender Systems?," A recommendation engine is a system, which on the basis of data analysis of the history of users and behaviour of similar users, suggests products, services, information to users. A recommendation can take user-user relationship, product-product relationships, product-user relationship etc. for recommendations."
 List out the libraries in Python used for Data Analysis and Scientific Computations.," The libraries NumPy, Scipy, Pandas, sklearn, Matplotlib which are most prevalent. For deep learning Pytorch, Tensorflow is great tools to learn."
 State the difference between the expected value and the mean value.," Mathematical expectation, also known as the expected value, is the summation or integration of possible values from a random variable. Mean value is the average of all data points."
 How are NumPy and SciPy related?, NumPy and SciPy are python libraries with support for arrays and mathematical functions. They are very handy tools for data science.
 What will be the output of the below Python code?, Error
 What do you mean by list comprehension?," List comprehension is an elegant way to define and create a list in Python. These lists often have the qualities of sets but are not in all cases sets. List comprehension is a complete substitute for the lambda function as well as the functions map(), filter(), and reduce()."
 What is __init__ in Python?, “__init__” is a reserved method in python classes. It is known as a constructor in object-oriented concepts. This method is called when an object is created from the class and it allows the class to initialise the attributes of the class.
 What is the difference between append() and extend() methods?, append() is used to add items to list. extend() uses an iterator to iterate over its argument and adds each element in the argument to the list and extends it.
" What is the output of the following? x =  [ ‘ab’, ‘cd’ ]   print(len(list(map(list, x))))", 2
 Write a Python program to count the total number of lines in a text file.,
 How will you read a random line in a file?,
 How would you effectively represent data with 5 dimensions?, It can be represented in a NumPy array of dimensions (n*n*n*n*5)
" Whenever you exit Python, is all memory de-allocated?", Objects having circular references are not always free when python exits. Hence when we exit python all memory doesn’t necessarily get deallocated.
 How would you create an empty NumPy array?,
 Treating a categorical variable as a continuous variable would result in a better predictive model?," There is no substantial evidence for that, but in some cases, it might help. It’s totally a brute force approach. Also, it only works when the variables in question are ordinal in nature."
 How and by what methods data visualisations can be effectively used?," Data visualisation is greatly helpful while creation of reports. There are quite a few reporting tools available such as tableau, Qlikview etc. which make use of plots, graphs etc for representing the overall idea and results for analysis. Data visualisations are also used in exploratory data analysis so that it gives us an overview of the data."
 You are given a data set consisting of variables with more than 30 per cent missing values. How will you deal with them?," If 30 per cent data is missing from a single column then, in general, we remove the column. If the column is too important to be removed we may impute values. For imputation, several methods can be used and for each method of imputation, we need to evaluate the model. We should stick with one that model which gives us the best results and generalises well to unseen data."
 What is skewed Distribution & uniform distribution?, The skewed distribution is a distribution in which the majority of the data points lie to the right or left of the centre. A uniform distribution is a probability distribution in which all outcomes are equally likely.
 What can be used to see the count of different categories in a column in pandas?, value_counts will show the count of different categories.
" What is the default missing value marker in pandas, and how can you detect all missing values in a DataFrame?", NaN is the missing values marker in pandas. All rows with missing values can be detected by is_null() function in pandas.
 What is root cause analysis?," Root cause analysis is the process of tracing back of occurrence of an event and the factors which lead to it. It’s generally done when a software malfunctions. In data science, root cause analysis helps businesses understand the semantics behind certain outcomes."
 What is a Box-Cox Transformation?," A Box Cox transformation is a way to normalise variables. Normality is an important assumption for many statistical techniques; if your data isn’t normal, applying a Box-Cox means that you are able to run a broader number of tests."
" What if instead of finding the best split, we randomly select a few splits and just select the best from them. Will it work?"," The decision tree is based on a greedy approach. It selects the best option for each branching. If we randomly select the best split from average splits, it would give us a locally best solution and not the best solution producing sub-par and sub-optimal results."
 What is the result of the below lines of code?, [1]
 How would you produce a list with unique elements from a list with duplicate elements?,
 How will you create a series from dict in Pandas?,
 How will you create an empty DataFrame in Pandas?,
 How to get the items of series A not present in series B?, We can do so by using series.isin() in pandas.
 How to get frequency counts of unique items of a series?, pandas.Series.value_counts gives the frequency of items in a series.
 How to convert a numpy array to a dataframe of given shape?, If matrix is the numpy array in question: df = pd.DataFrame(matrix) will convert matrix into a dataframe.
 What is Data Aggregation?," Data aggregation is a process in which aggregate functions are used to get the necessary outcomes after a groupby. Common aggregation functions are sum, count, avg, max, min."
 What is Pandas Index?, An index is a unique number by which rows in a pandas dataframe are numbered.
 Describe Data Operations in Pandas?," Common data operations in pandas are data cleaning, data preprocessing, data transformation, data standardisation, data normalisation, data aggregation."
 Define GroupBy in Pandas?, groupby is a special function in pandas which is used to group rows together given certain specific columns which have information for categories used for grouping data together.
 How to convert the index of a series into a column of a dataframe?, df = df.reset_index() will convert index to a column in a pandas dataframe.
 How to keep only the top 2 most frequent values as it is and replace everything else as ‘Other’?,
 How to convert the first character of each element in a series to uppercase?, pd.Series([x.title() for x in s])
" How to get the minimum, 25th percentile, median, 75th, and max of a numeric series?",
 What kind of data does Scatterplot matrices represent?, Scatterplot matrices are most commonly used to visualise multidimensional data. It is used in visualising bivariate relationships between a combination of variables.
 What is the hyperbolic tree?, A hyperbolic tree or hypertree is an information visualisation and graph drawing method inspired by hyperbolic geometry.
 What is scientific visualisation? How it is different from other visualisation techniques?, Scientific visualization is representing data graphically as a means of gaining insight from the data. It is also known as visual data analysis. This helps to understand the system that can be studied in ways previously impossible.
 What are some of the downsides of Visualisation?," Few of the downsides of visualisation are: It gives estimation not accuracy, a different group of the audience may interpret it differently, Improper design can cause confusion."
 What is the difference between a tree map and heat map?, A heat map is a type of visualisation tool that compares different categories with the help of colours and size. It can be used to compare two different measures.                                                                                             The ‘tree map’ is a chart type that illustrates hierarchical data or part-to-whole relationships.
 What is disaggregation and aggregation of data?," Aggregation basically is combining multiple rows of data at a single place from low level to a higher level. Disaggregation, on the other hand, is the reverse process i.e breaking the aggregate data to a lower level."
 What are some common data quality issues when dealing with Big Data?," Some of the major quality issues when dealing with big data are duplicate data, incomplete data, the inconsistent format of data, incorrect data, the volume of data(big data), no proper storage mechanism, etc."
 What is clustering?," Clustering means dividing data points into a number of groups. The division is done in a way that all the data points in the same group are more similar to each other than the data points in other groups. A few types of clustering are Hierarchical clustering, K means clustering, Density-based clustering, Fuzzy clustering etc."
 What are the data mining packages in R?," A few popular data mining packages in R are Dplyr- data manipulation, Ggplot2- data visualisation, purrr- data wrangling, Hmisc- data analysis, datapasta- data import etc."
 What are techniques used for sampling? Advantage of sampling,"There are various methods for drawing samples from data. The two main Sampling techniques are Probability sampling Probability sampling means that each individual of the population has a possibility of being included in the sample. Probability sampling methods include – In simple random sampling, each individual of the population has an equivalent chance of being selected or included. Systematic sampling is very much similar to random sampling. The difference is just that instead of randomly generating numbers, in systematic sampling every individual of the population is assigned a number and are chosen at regular intervals. In stratified sampling, the population is split into sub-populations. It allows you to conclude more precise results by ensuring that every sub-population is represented in the sample. Cluster sampling also involves dividing the population into sub-populations, but each subpopulation should have analogous characteristics to that of the whole sample. Rather than sampling individuals from each subpopulation, you randomly select the entire subpopulation. Non-probability sampling In non-probability sampling, individuals are selected using non-random ways and not every individual has a possibility of being included in the sample. Convenience sampling is a method where data is collected from an easily accessible group. Purposive sampling also known as judgmental sampling is where the researchers use their expertise to select a sample that is useful or relevant to the purpose of the research. Snowball sampling is used where the population is difficult to access. It can be used to recruit individuals via other individuals. Advantages of Sampling"
 What is imbalance data?,"Imbalance data in simple words is a reference to different types of datasets where there is an uneven distribution of observations to the target class.  Which means, one class label has higher observations than the other comparatively."
" Define Lift, KPI, Robustness, Model fitting and DOE","Lift is used to understand the performance of a given targeting model in predicting performance, when compared against a randomly picked targeting model. KPI or Key performance indicators is a yardstick used to measure the performance of an organization or an employee based on organizational objectives. Robustness is a property that identifies the effectiveness of an algorithm when tested with a new independent dataset. Model fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained. Design of Experiment (DOE) is a set of mathematical methods for process optimization and for quality by design (QbD)."
 Define Confounding Variables,"A confounding variable is an external influence in an experiment. In simple words, these variables change the effect of a dependent and independent variable. A variable should satisfy below conditions to be a confounding variable : For example, if you are studying whether a lack of exercise has an effect on weight gain, then the lack of exercise is an independent variable and weight gain is a dependent variable. A confounder variable can be any other factor that has an effect on weight gain. Amount of food consumed, weather conditions etc. can be a confounding variable."
 Why are time series problems different from other regression problems?,"Time series is extrapolation whereas Regression is interpolation. Time-series refers to an organized chain of data. Time-series forecasts what comes next in the sequence. Time-series could be assisted with other series which can occur together. Regression can be applied to Time-series problems as well as to non-ordered sequences which are termed as Features. While making a projection, new values of Features are presented and Regression calculates results for the target variable."
 What is the difference between the Test set and validation set?,"Test set : Test set is a set of examples used only to evaluate the performance of a fully specified classifier. In simple words, it is used to fit the parameters. It is used to test the data which is passed as input to your model. Validation set : Validation set is a set of examples used to tune the parameters of a classifier. In simple words, it is used to tune the parameters. Validation set is used to validate the output which is produced by your model. Kernel Trick A Kernel Trick is a method where a linear classifier is used to solve non-linear problems. In other words, it is a method where a non-linear object is projected to a higher dimensional space to make it easier to categorize where the data would be divided linearly by a plane. Let’s understand it better, Let’s define a Kernel function K as xi and xj as just being the dot product. K(xi,xj) = xi . xj = xTixj If every data point is mapped into the high-dimensional space via some transformation Φ:x -> Φ(x) The dot product becomes: K(xi,xj) = ΦxTiΦxj Box Plot and Histograms Box Plot and Histogram are types of charts that represent numerical data graphically. It is an easier way to visualize data. It makes it easier to compare characteristics of data between categories."
Supervised Learning vs. Unsupervised Learning: What’s the Difference?,"Supervised and unsupervised learning systems differ in the nature of the training data that they’re given. Supervised learning requires labeled training data, whereas, in unsupervised learning, the system is provided with unlabeled data and discovers the trends that are present."
What Is Logistic Regression?,Logistic regression is a form of predictive analysis. It is used to find the relationships that exist between a dependent binary variable and one or more independent variables by employing a logistic regression equation.
What Is a Decision Tree?,"Decision trees are a tool used to classify data and determine the possibility of defined outcomes in a system. The base of the tree is known as the root node. The root node branches out into decision nodes based on the various decisions that can be made at each stage. Decision nodes flow into lead nodes, which represent the consequence of each decision."
What Is Pruning in a Decision Tree Algorithm?,"Pruning a decision tree is the process of eliminating non-critical subtrees so that the data under consideration is not overfitted. In pre-pruning, the tree is pruned as it is being constructed, following criteria like the Gini index or information gain metrics. Post-pruning entails pruning a tree from the bottom up after it has been constructed."
What Is Entropy in a Decision Tree Algorithm?,"Entropy is a measure of the level of uncertainty or impurity that’s present in a dataset. For a dataset with N classes, the entropy is described by the following formula."
Explain K-Fold Cross-Validation.,"Cross-validation is a technique used to estimate the efficacy of a machine learning model. The parameter, k, is a tally of the number of groups that a dataset can be split up into. The process starts with the entire dataset being shuffled in a random manner. It is then divided into k groups, also known as folds. The following procedure is applied to each unique fold:"
Explain the Random Forest Model. How Do You Build a Random Forest Model?,A random forest model is a machine learning algorithm and a form of supervised learning. It is used most commonly in regression and classification problems. Here are the steps to build a random forest model:
"What Is the Difference Between Univariate, Bivariate, and Multivariate Analysis?","Univariate analysis involves studying a single variable. Bivariate and multivariate analysis involve comparing two, or more than two variables, respectively."
"Can You Avoid Overfitting Your Model? If Yes, Then How?","Yes, it is possible to overfit data models. The following techniques can be used for that purpose."
What Feature Selection Methods Are Used To Select the Right Variables?,The following are some of the techniques used for feature selection in data analysis:
How Would You Approach a Dataset That’s Missing More Than 30 Percent of Its Values?,"The approach will depend on the size of the dataset. If it is a large dataset, then the quickest method would be to simply remove the rows containing the missing values. Since the dataset is large, this won’t affect the ability of the model to produce results. If the dataset is small, then it is not practical to simply eliminate the values. In that case, it is better to calculate the mean or mode of that particular feature and input that value where there are missing entries. Another approach would be to use a machine learning algorithm to predict the missing values. This can yield accurate results unless there are entries with a very high variance from the rest of the dataset."
Explain Dimensionality Reduction and Its Benefits.,Dimensionality reduction is the process of eliminating the redundant variables or features being studied in a machine learning environment. The benefits of dimensionality reduction are:
What Are the Steps Involved in Maintaining a Deployed Model?,The following measures should be taken to maintain data analysis models once they have been deployed:
How Can You Calculate Euclidean Distance in Python?,"There are multiple inbuilt modules and functions that you can use to calculate Euclidean distance in Python. That includes the NumPy module, math.dist() function, and distance.euclidean() function. The following code shows how to use the distance.euclidean() function to calculate Euclidean distance."
Explain What a Recommender System Does.,"A recommender system uses historical behavior to predict how a user will rate a specific item. For example, Netflix recommends TV shows and movies to users by analyzing the media that users have rated in the past, and using this to recommend new media that they might like."
What Is the RMSE?,Root mean squared error (RMSE) is a metric that calculates the error in a numerical prediction. The following is the formula for RMSE.
How Do You Calculate the MSE in a Linear Regression Model?,Mean squared error (MSE) is a measure of the degree of error that is present in a statistical model. It can be found with the following formula:
What Is K-Means Clustering?,K-means is an unsupervised learning algorithm used for problems having to do with clustering data. It follows the sequence of steps described below:
How Can You Select K for K-Means?,"The most popular method for selecting k for the k-means algorithm is using the elbow method. To do this, you need to calculate the Within-Cluster-Sum of Squared Errors (WSS) for different k values. The WSS is described as the sum of the squares of the distance between each data value and its centroid. You will then choose the value of k for which the WSS error starts to become negligible."
What Is a P-Value? What Is the Significance of P-Value?,"P-value expresses the probability that an observation made about a dataset is a random chance. Any p-value under 5% is strong evidence supporting the observation and against the null hypothesis. The higher the p-value, the less likely that a result is valid."
What Is an Outlier?,An outlier is a data value that lies at a great distance from the other values in a dataset. An outlier might be the result of an experimental error or a valid value that shows a high degree of variance from the mean.
How Do You Treat Outlier Values?,"Outlets are often filtered out during data analysis if they don’t fit certain criteria. You can set up a filter in the data analysis tool you’re using to automatically eliminate outliers. However, there are instances where outliers can reveal insights about low-percentage possibilities. In that case, analysts might group outliers and study them separately."
Explain Normal Distribution,A normal distribution is a probability distribution where the values are symmetric on either side of the mean of the data. This implies that values closer to the mean are more common than values that are further away from it.
What Is Deep Learning?,"Deep learning is a subset of machine learning concerned with supervised, unsupervised, and semi-supervised learning based on artificial neural networks."
What Is an RNN (Recurrent Neural Network)?,A recurrent neural network is a kind of artificial neural network where the connections between nodes are based on a time series. RNNs are the only form of neural network with internal memory and are often used for speech recognition applications.
Explain the ROC Curve.,ROC curves are graphs that depict how a classification model performs at different classification thresholds. The graph is plotted with the True Positive Rate (TPR) on the y-axis and the False Positive Rate (FPR) on the x-axis. The TPR is expressed as the ratio between the number of true positives and the sum of the number of true positives and false negatives. The FPR is the ratio between the number of false positives in a dataset and the sum of the number of false positives and true negatives.
What Is the Difference Between Data Modeling and Database Design?,"A data model is a conceptual model showing the different entities from which data is sourced and the relationships between them. Database design, on the other hand, is the process of building a schema based on how a database is constructed."
Explain Time Series Analysis.,A time-series analysis is a form of data analysis that looks at data values collected in a particular sequence. It both studies the data collected over time and factors in the different points in time in which data was collected.
How Can Time-Series Data Be Declared As Stationery?,Time series data being declared stationary implies that the data being collected does not change over time. This may be because there are no time-based or seasonal trends in the data.
What Is a Confusion Matrix?,"A confusion matrix is used to determine the efficacy of a classification algorithm. It is used because a classification algorithm isn’t accurate when there are more than two classes of data, or when there isn’t an even number of classes. The process for creating a confusion matrix is as follows: The matrix that results from this process is known as a confusion matrix."
How Do You Use a Confusion Matrix To Calculate Accuracy?,"There are four terms to be aware of related to confusion matrices. They are: True positives (TP): When a positive outcome was predicted, and the result came positive True negatives (TN): What a negative outcome was predicted, and the result turned out negative False positives (FP): When a positive outcome was predicted, but the result is negative False negative (FN): When a negative outcome was predicted, but the result is positive The accuracy of a model can be calculated using a confusion matrix using the formula: Accuracy = TP + TN / TP + TN + FP + FN"
Write the Equations for Precision and Recall Rate.,"The precision of a model is given by: Precision = True Positives / (True Positives + False Positives) The recall rate for a model is given by: Recall = True Positives / (True Positives + False Negatives) A recall rate of 1 implies full recall, and that of 0 means that there is no recall."
What Is the Use of the Summary Function?,"Summary functions summarize the results of different model-fitting functions. In R, for example, the summary() function can be used to gather a quick overview of your dataset and the results that are produced by a machine learning algorithm."
How Do You Differentiate Between an Error and a Residual Error?,"Error is a measure of the extent to which an observed value deviates from a true value. Residual error, on the other hand, expresses how much an observed value differs from the estimated value of a particular data point."
“People Who Bought This Also Bought…” or “You May Also Like…” Recommendations Seen on Amazon Are a Result of Which Algorithm?,Those recommendations are produced using item-based collaborative filtering algorithms.
What Is an SQL Query? Write a Basic SQL Query That Lists All Orders With Customer Information.,"An SQL query is a request that returns a particular kind of data from a database. For example, let’s say you want to write an SQL that lists all the orders currently in a database along with information on the customers who made those orders. The SQL query for that request would look as follows: SELECT Orders.CustomerID, Orders.RequiredDate, Orders.ShipVia, Orders.ShipPostalCode, Customers.Address, Customers.Region, Customers.CompanyName FROM Customers This query takes data from a table called “Customers.” It returns entries with information on the customer’s identification number, address, region, company name, postal code, and shipping details. To help you prepare, we have compiled a post with the 105 most asked SQL interview questions and their answers."
K-Means Clustering vs. Linear Regression vs. K-NN (K-Nearest Neighbor) vs. Decision Trees: Which Machine Learning Algorithms Can Be Used for Inputting Missing Values of Both Categorical and Continuous Variables?,K-NN algorithms work best when it comes to inputting values in categorical and continuous data.
Are Data Science and Machine Learning Related to Each Other?,"Data science and machine learning are closely related, and many machine learning algorithms are used in data science. Data science is the extraction of useful insights from large volumes of data. Machine learning is the process of training algorithms to derive automated insights."
Explain Ensemble Learning.,Ensemble learning is a machine learning practice in which multiple models are used to improve the predictive performance of a data analysis model.
What Do You Mean by Bagging?,Bagging is an ensemble learning technique used to reduce the amount of variance in a noisy dataset.
Explain Boosting in Data Science.,Boosting is an ensemble learning technique used to strengthen a weak learning model.
Explain Naive Bayes.,"Naive Bayes is a classification algorithm that works on the assumption that every feature under consideration is independent. It is called naive because of that very same assumption, which is often unrealistic for data in the real world. However, it does tend to work well to solve a large range of problems."
Which Is Better for Text Analytics—Python or R?,"Both Python and R can be used to analyze text. R comes with several in-built libraries for text analysis, as does Python. Their differences come down to the nature of the data being studied. Python is better when working with huge volumes of data. R has better support for unstructured data."
What Is an SVM in Data Science?,"SVMS—or support vector machines—are used for predictive or classification tasks. They employ what are known as hyperplanes to differentiate two different variable classes. Polynomial kernels, Gaussian kernels, and Sigmoid kernels are some of the kernels used in SVM."
What Is Data Science?,Data science is the process of using various mathematical and computational techniques to extract meaningful insights from datasets.
Data Science vs. Data Analytics: What’s the Difference?,Data science uses insights extracted from data to solve specific business problems. Data analytics is a more exploratory practice of unearthing the correlations and patterns that exist in a dataset.
Why Did You Opt for a Data Science Career?,"Tell them how you got passionate about data science. You can share a quick story or talk about a specific area that served as your gateways to data science, such as statistical analysis or Python programming. Then, talk about your background—your college degree, previous companies you’ve worked at, and data science courses that you’ve completed. Finally, relate your interests to the organization’s needs,  and explain how your expertise in data science can help the company solve its challenges. Related Read: 30 Statistics Interview Questions to Prep For Your Interview"
What Is a Statistical Interaction?,"A statistical interaction is when two or more variables interact, and this results in a third variable being affected."
Explain Linear Regression.,"Linear regression is a tool for quick predictive analysis. For example, the price of a house depends on a myriad of factors, including its size and location. In order to see the relationship between these variables, you can build a linear regression, which predicts the line of best fit and can help conclude whether or not these two factors have a positive or negative relationship."
What Are the Assumptions Required for a Linear Regression?,"There are four major assumptions. 1. There is a linear relationship between the dependent variables and the regressors, meaning the model you are creating actually fits the data. 2. The errors or residuals of the data are normally distributed and independent from each other. 3. There is minimal multicollinearity between explanatory variables 4. Homoscedasticity—the variance around the regression line—is the same for all values of the predictor variable."
What Do You Understand About the True-Positive Rate and False-Positive Rate?,The true-positive rate (TPR) is the ratio between the number of true positives (TP) and the sum of the number of true positives and false negatives (FN). The false positive rate (FPR) is the ratio between the number of false positives and the sum of the number of false positives and true negatives. TPR = TP/TP+FN FPR = FP/FP+TN
Can You Differentiate Between Long-Format Data and Wide-Format Data?,These are two different ways in which a dataset can be written. Long-format data implies that data values in the first column do repeat. Wide-format means that there are no values that repeat in the first column.
"In Data Science, Why Is Python Used for Data Cleaning?","Python is used for data cleaning because it includes libraries like NumPy and Pandas, which make it simple to eliminate inaccurate data values."
Explain Data Visualization.,"Data visualization is the process of converting numerical and textual data insights into a visual format. Graphs, charts, tables, and other aids are used to make data visualization possible."
Why Is R Used in Data Visualization?,"R has a wide offering of packages for data visualization. These require a minimal amount of coding, which is why R is a popular language for data visualizations."
List Some Popular Libraries Used in Data Science.,"TensorFlow, Matplotlib, Keras, SciPy, and PyTorch are popular libraries used in data science."
What Is Variance in Data Science?,Variance is the distance between a value in a dataset and the mean value.
Explain Feature Vectors.,A feature vector describes the features of an object under consideration.
Explain Root Cause Analysis.,Root cause analysis is the process of using data to discover the underlying patterns driving a certain change.
What Is Logistic Regression?,Logistic regression is a form of regression analysis. It is used to establish the correlation between a dependent binary variable and one or many independent variables. Logistic regression is used as a predictive analytical tool.
What Is an Example of a Data Set With a Non-Gaussian Distribution?,An example of this would be the distribution of height in a population.
What Is Cross-Validation?,Cross-validation is a technique that demonstrates the efficacy of a machine learning model when analyzing unseen data. It tells us whether the model performs as well on test data as it did on training data.
What Is Collaborative Filtering?,Collaborative filtering is a form of content filtering that uses similarities between different users to make recommendations
Why Is A/B Testing Conducted?,"A/B testing gives businesses the ability to peek into customers’ minds and get an idea of their preferences. You can quantify the amount of interest that different offerings garner in test groups, which lets you go to market with the final product with confidence."
What Is a Linear Regression Model? List Its Drawbacks.,A linear regression model is a model in which there is a linear relationship between the dependent and independent variables. Here are the drawbacks of linear regression:
What Is the Law of Large Numbers?,"This law of probability states that, to get close to an expected result, you should run an experiment a large number of times, each independent of the other, and then average out the result."
Explain Confounding Variables.,"When trying to investigate the relationship between a cause and its purported effect, you might encounter a third variable that impacts both the cause and the effect. This is known as a confounding variable."
Do Gradient Descent Methods Always Converge to the Same Point? Why or Why Not?,"They don’t. This is because, in some cases, they settle on the locally optimal point rather than a global minima."
What Is a Star Schema?,"A star schema is a way of structuring a database that stores measured data in a single fact table. It is called a star schema because the main table sits at the center of a logical diagram, and the smaller tables branch off like the nodes in a star."
Why Must You Update an Algorithm Regularly? How Frequently Should You Update It?,"It is important to keep tweaking your machine learning algorithms regularly. The frequency with which you update them will depend on the business use case. For example, fraud detection algorithms need to be updated regularly. But if you need to study manufacturing data using machine learning, then those models need to be updated much less regularly."
What Are an Eigenvalue and an Eigenvector?,"An eigenvector produces another vector in the same direction, but with an increased magnitude. The degree to which the eigenvector becomes scaled up is determined by a metric known as the eigenvalue."
Why Is Sampling Conducted?,Sampling is a statistical technique where a representative subset of a larger dataset is analyzed to infer trends.
Mention Some Techniques Used for Sampling. What Is the Main Advantage of Sampling?,"The following are commonly used sampling techniques: Sampling is more cost- and time-efficient than studying a full dataset. It lets you analyze a subset of that data, which is easier while providing insights into the whole dataset."
What Is Bias in Data Science?,The bias present in a data science model is described by the difference between the predicted value that it produces and a target value obtained from training data.
Explain Selection Bias.,Selection bias occurs when the sample data extracted from a larger dataset isn’t fully representative. This leads to faulty conclusions being made about the dataset.
What Is Survivorship Bias?,"Survivorship bias occurs when there is too much focus placed on data that survived a particular selection process, while ignoring the data that did not survive it."
How Do You Work Towards a Random Forest?,The following algorithm is used to construct a random forest:
What Is the Binomial Probability Formula?,Binomial probability measures the number of successes that will occur when a certain number of trials is conducted. It is given by the following formula:
What Is the Difference Between a Type I and Type II Error?,"A type I error is a false positive, which means that a positive result was predicted, but the result is negative. A type II error is a false negative, which means that a negative result was predicted, but the actual result is positive."
Introduce Yourself.,"This is an opportunity to tell interviewers about your background and how got you interested in data science. You can describe notable moments in your data science career, like personal projects or awards, to make an impact on interviewers."
What Do You Know About Data Science?,"This might seem like an invitation to talk about everything you know about data science, but it isn’t. Rather, recruiters want to know whether you understand the foundations of the discipline and how it fits into a business context. Start by defining data science. Describe why it has gained importance as a field and how businesses can benefit from it. If possible, tailor this answer to the company where you’re interviewing and explain how data science can be used to solve the types of questions they want answers to."
Why Did You Opt for a Data Science Career?,Recruiters ask this question to gauge whether candidates are genuinely passionate about data science. Your personal story is a powerful tool here. Be genuine about your interest in data science. Do you enjoy it because you love programming for data analysis? Or did you fall in love with it when you decided to take a data science course on a whim? Be honest about your journey and emphasize anything that illustrates your passion for the field.
What Is the Most Challenging Project You Encountered on Your Learning Journey?,"Start by describing the project that you were working on. What problem were you trying to solve, and how did you translate it into requirements for a data analysis project? Then describe your process and how you solved the problems that you faced along the way. Focus on your problem-solving approaches and how you did the research required to overcome the challenges you faced."
Situational question based on the resume.,"If you have a gap in your resume, recruiters will often ask about it. There’s no need to panic. Just be honest about why you took a professional break, and explain how you’ve gotten reacquainted with the industry. Candidates without an academic background in computer science or math might get asked why they didn’t pursue those fields. Answer this question by explaining why you chose an unconventional route. Further reading: here is a guide with data science interview preparation tips to know what to expect from your data science interview"
Is Data Science Hard To Learn?,"No. Anyone with the desire and commitment can learn data science. There are plenty of resources for beginners, and there are also courses and bootcamps where you can study data science. The math you’ll need as a beginner is quite foundational."
Is Data Science a Good Career?,"Yes. There is a huge demand for data scientists in various industries, and salaries have also grown commensurately. Data science can also give you the opportunity to contribute to your company in meaningful ways."
How Long Does It Take To Transition Into Data Science?,"If you have a background in math or computer science, then you can transition into data science easily. But if you don’t have this background, then you should give yourself at least six months to get familiar with the math and coding skills that are required."
"If an Interviewer Asks, “Why Should We Hire You as a Data Scientist?” Then How Should You Answer?","Explain what makes you a skilled data scientist by describing the different data analysis approaches and tools you’re familiar with. Then, talk about the needs of the company and how you can help solve its most pressing challenges by leveraging those skills. Since you’re here…Are you interested in this career track? Investigate with our free guide to what a data professional actually does. When you’re ready to build a CV that will make hiring managers melt, join our Data Science Bootcamp which will help you land a job or your tuition back!"
Five Reasons Why Data Science is the Career of the Future,
Data Engineer vs. Data Scientist: The Best Choice for 2024,
How To Become a Big Data Engineer: 2024 Guide,
 What are the differences between supervised and unsupervised learning?,Become a Data Scientist with Hands-on Training!Data Scientist Masterâs ProgramExplore Program
 How is logistic regression done?,Logistic regression measures the relationship between the dependent variable (our label of what we want to predict) and one or more independent variables (our features) by estimating probability using its underlying logistic function (sigmoid). The image shown below depicts how logistic regression works:  The formula and graph for the sigmoid function are as shown:
 Explain the steps in making a decision tree.,"For example, let's say you want to build a decision tree to decide whether you should accept or decline a job offer. The decision tree for this case is as shown:  It is clear from the decision tree that an offer is accepted if:"
 How do you build a random forest model?,"A random forest is built up of a number of decision trees. If you split the data into different packages and make a decision tree in each of the different groups of data, the random forest brings all those trees together."
 How can you avoid overfitting your model?,Overfitting refers to a model that is only set for a very small amount of data and ignores the bigger picture. There are three main methods to avoid overfitting: Learn Job Critical Skills To Help You Grow!Big Data Engineer Masterâs ProgramExplore Program
"Differentiate between univariate, bivariate, and multivariate analysis.","Univariate data contains only one variable. The purpose of the univariate analysis is to describe the data and find patterns that exist within it.Â Example: height of studentsÂ The patterns can be studied by drawing conclusions using mean, median, mode, dispersion or range, minimum, maximum, etc. Bivariate data involves two different variables. The analysis of this type of data deals with causes and relationships and the analysis is done to determine the relationship between the two variables. Example: temperature and ice cream sales in the summer season Here, the relationship is visible from the table that temperature and sales are directly proportional to each other. The hotter the temperature, the better the sales. Multivariate data involves three or more variables, it is categorized under multivariate. It is similar to a bivariate but contains more than one dependent variable. Example: data for house price predictionÂ The patterns can be studied by drawing conclusions using mean, median, and mode, dispersion or range, minimum, maximum, etc. You can start describing the data and using it to guess what the price of the house will be."
 What are the feature selection methods used to select the right variables?,"There are two main methods for feature selection, i.e, filter, and wrapper methods. This involves:Â The best analogy for selecting features is ""bad data in, bad answer out."" When we're limiting or selecting the features, it's all about cleaning up the data coming in.Â This involves:Â Wrapper methods are very labor-intensive, and high-end computers are needed if a lot of data analysis is performed with the wrapper method.Â Future-Proof Your AI/ML Career: Top Dos and Don'tsFree Webinar | 5 Dec, Tuesday | 7 PM ISTRegister Now"
" In your choice of language, write a program that prints the numbers ranging from one to 50.","But for multiples of three, print ""Fizz"" instead of the number, and for the multiples of five, print ""Buzz."" For numbers which are multiples of both three and five, print ""FizzBuzz""Â The code is shown below:  Note that the range mentioned is 51, which means zero to 50. However, the range asked in the question is one to 50. Therefore, in the above code, you can include the range as (1,51). The output of the above code is as shown:"
 You are given a data set consisting of variables with more than 30 percent missing values. How will you deal with them?,"The following are ways to handle missing data values: If the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; we use the rest of the data to predict the values. For smaller data sets, we can substitute missing values with the mean or average of the rest of the data using the pandas' data frame in python. There are different ways to do so, such as df.mean(), df.fillna(mean)."
" For the given points, how will you calculate the Euclidean distance in Python?","plot1 = [1,3] plot2 = [2,5] The Euclidean distance can be calculated as follows: euclidean_distance = sqrt( (plot1[0]-plot2[0])**2 + (plot1[1]-plot2[1])**2 ) Check out the Simplilearn's video on ""Data Science Interview Question"" curated by industry experts to help you prepare for an interview."
 What are dimensionality reduction and its benefits?,"The Dimensionality reduction refers to the process of converting a data set with vast dimensions into data with fewer dimensions (fields) to convey similar information concisely.Â This reduction helps in compressing data and reducing storage space. It also reduces computation time as fewer dimensions lead to less computing. It removes redundant features; for example, there's no point in storing a value in two different units (meters and inches).Â"
 How will you calculate eigenvalues and eigenvectors of the following 3x3 matrix?,"The characteristic equation is as shown: Expanding determinant: (-2 â Î») [(1-Î») (5-Î»)-2x2] + 4[(-2) x (5-Î») -4x2] + 2[(-2) x 2-4(1-Î»)] =0 - Î»3 + 4Î»2 + 27Î» â 90 = 0, Î»3 - 4 Î»2 -27 Î» + 90 = 0 Here we have an algebraic equation built from the eigenvectors. By hit and trial: 33 â 4 x 32 - 27 x 3 +90 = 0 Hence, (Î» - 3) is a factor: Î»3 - 4 Î»2 - 27 Î» +90 = (Î» â 3) (Î»2 â Î» â 30) Eigenvalues are 3,-5,6: (Î» â 3) (Î»2 â Î» â 30) = (Î» â 3) (Î»+5) (Î»-6), Calculate eigenvector for Î» = 3 For X = 1, -5 - 4Y + 2Z =0, -2 - 2Y + 2Z =0 Subtracting the two equations:Â 3 + 2Y = 0, Subtracting back into second equation: Y = -(3/2)Â Z = -(1/2) Similarly, we can calculate the eigenvectors for -5 and 6."
 How should you maintain a deployed model?,"The steps to maintain a deployed model are: Constant monitoring of all models is needed to determine their performance accuracy. When you change something, you want to figure out how your changes are going to affect things. This needs to be monitored to ensure it's doing what it's supposed to do. Evaluation metrics of the current model are calculated to determine if a new algorithm is needed.Â The new models are compared to each other to determine which model performs the best.Â The best-performing model is re-built on the current state of data."
 What are recommender systems?,"A recommender system predicts what a user would rate a specific product based on their preferences. It can be split into two different areas: As an example, Last.fm recommends tracks that other users with similar interests play often. This is also commonly seen on Amazon after making a purchase; customers may notice the following message accompanied by product recommendations: ""Users who bought this also boughtâ¦"" As an example: Pandora uses the properties of a song to recommend music with similar properties. Here, we look at content, instead of looking at who else is listening to music."
 How do you find RMSE and MSE in a linear regression model?,RMSE and MSE are two of the most common measures of accuracy for a linear regression model.Â RMSE indicates the Root Mean Square Error.Â  MSE indicates the Mean Square Error.  Become a Data Scientist with Hands-on Training!Data Scientist Masterâs ProgramExplore Program
 How can you select k for k-means?Â,"We use the elbow method to select k for k-means clustering. The idea of the elbow method is to run k-means clustering on the data set where 'k' is the number of clusters. Within the sum of squares (WSS), it is defined as the sum of the squared distance between each member of the cluster and its centroid.Â"
 What is the significance of p-value?,"p-value typically â¤ 0.05 This indicates strong evidence against the null hypothesis; so you reject the null hypothesis. p-value typically > 0.05 This indicates weak evidence against the null hypothesis, so you accept the null hypothesis.Â p-value at cutoff 0.05Â This is considered to be marginal, meaning it could go either way."
 How can outlier values be treated?,"You can drop outliers only if it is a garbage value.Â Example: height of an adult = abc ft. This cannot be true, as the height cannot be a string value. In this case, outliers can be removed. If the outliers have extreme values, they can be removed. For example, if all the data points are clustered between zero to 10, but one point lies at 100, then we can remove this point. If you cannot drop outliers, you can try the following:"
 How can time-series data be declared as stationery?,"It is stationary when the variance and mean of the series are constant with time.Â Here is a visual example:Â  In the first graph, the variance is constant with time. Here, X is the time factor and Y is the variable. The value of Y goes through the same points all the time; in other words, it is stationary. In the second graph, the waves get bigger, which means it is non-stationary and the variance is changing with time."
 How can you calculate accuracy using a confusion matrix?,"Consider this confusion matrix:  You can see the values for total data, actual values, and predicted values. The formula for accuracy is: Accuracy = (True Positive + True Negative) / Total Observations = (262 + 347) / 650 = 609 / 650 = 0.93 As a result, we get an accuracy of 93 percent."
 Write the equation and calculate the precision and recall rate.,Consider the same confusion matrix used in the previous question.  Precision = (True positive) / (True Positive + False Positive) = 262 / 277 = 0.94 Recall Rate = (True Positive) / (Total Positive + False Negative) = 262 / 288 = 0.90
 'People who bought this also boughtâ¦' recommendations seen on Amazon are a result of which algorithm?,"The recommendation engine is accomplished with collaborative filtering. Collaborative filtering explains the behavior of other users and their purchase history in terms of ratings, selection, etc.Â The engine makes predictions on what might interest a person based on the preferences of other users. In this algorithm, item features are unknown. Â For example, a sales page shows that a certain number of people buy a new phone and also buy tempered glass at the same time. Next time, when a person buys a phone, he or she may see a recommendation to buy tempered glass as well."
 Write a basic SQL query that lists all orders with customer information.,"Usually, we have order tables and customer tables that contain the following columns:"
 You are given a dataset on cancer detection. You have built a classification model and achieved an accuracy of 96 percent. Why shouldn't you be happy with your model performance? What can you do about it?,"Cancer detection results in imbalanced data. In an imbalanced dataset, accuracy should not be based as a measure of performance. It is important to focus on the remaining four percent, which represents the patients who were wrongly diagnosed. Early diagnosis is crucial when it comes to cancer detection, and can greatly improve a patient's prognosis. Hence, to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine the class wise performance of the classifier. Become a Data Scientist with Hands-on Training!Data Scientist Masterâs ProgramExplore Program"
 Which of the following machine learning algorithms can be used for inputting missing values of both categorical and continuous variables?,"The K nearest neighbor algorithm can be used because it can compute the nearest neighbor and if it doesn't have a value, it just computes the nearest neighbor based on all the other features.Â When you're dealing with K-means clustering or linear regression, you need to do that in your pre-processing, otherwise, they'll crash. Decision trees also have the same problem, although there is some variance."
 Below are the eight actual values of the target variable in the train file. What is the entropy of the target variable?,"[0, 0, 0, 1, 1, 1, 1, 1]Â Choose the correct answer. The target variable, in this case, is 1.Â The formula for calculating the entropy is: Putting p=5 and n=8, we getÂ Entropy = A = -(5/8 log(5/8) + 3/8 log(3/8))"
" We want to predict the probability of death from heart disease based on three risk factors: age, gender, and blood cholesterol level. What is the most appropriate algorithm for this case?","Choose the correct option: The most appropriate algorithm for this case is A, logistic regression.Â"
" After studying the behavior of a population, you have identified four specific individual types that are valuable to your study. You would like to find all users who are most similar to each individual type. Which algorithm is most appropriate for this study?","Choose the correct option: As we are looking for grouping people together specifically by four different similarities, it indicates the value of k. Therefore, K-means clustering (answer A) is the most appropriate algorithm for this study."
" You have run the association rules algorithm on your dataset, and the two rules {banana, apple} => {grape} and {apple, orange} => {grape} have been found to be relevant. What else must be true?","Choose the right answer: The answer is A: {grape, apple} must be a frequent itemset"
 Your organization has a website where visitors randomly receive one of two coupons. It is also possible that visitors to the website will not receive a coupon. You have been asked to determine if offering a coupon to website visitors has any impact on their purchase decisions. Which analysis method should you use?,The answer is A: One-way ANOVA
 What do you understand about true positive rate and false-positive rate?,The True Positive Rate (TPR) is calculated by taking the ratio of the [True Positives (TP)] and [True Positive (TP) & False Negatives (FN) ].Â The formula for the same is stated below - TPR=TP/TP+FN The False Positive Rate (FPR) is calculated by taking the ratio of the [False Positives (FP)] and [True Positives (TP) & False Positives(FP)]. The formula for the same is stated below - FPR=FP/TN+FP
 What is the ROC curve?,"The graph between the True Positive Rate on the y-axis and the False Positive Rate on the x-axis is called the ROC curve and is used in binary classification. The False Positive Rate (FPR) is calculated by taking the ratio between False Positives and the total number of negative samples, and the True Positive Rate (TPR) is calculated by taking the ratio between True Positives and the total number of positive samples. In order to construct the ROC curve, the TPR and FPR values are plotted on multiple threshold values. The area range under the ROC curve has a range between 0 and 1. A completely random model, which is represented by a straight line, has a 0.5 ROC. The amount of deviation a ROC has from this straight line denotes the efficiency of the model.  The image above denotes a ROC curve example."
 What is a Confusion Matrix?,The Confusion Matrix is the summary of prediction results of a particular problem. It is a table that is used to describe the performance of the model. The Confusion Matrix is an n*n matrix that evaluates the performance of the classification model.
 What do you understand about the true-positive rate and false-positive rate?,TRUE-POSITIVE RATE: The true-positive rate gives the proportion of correct predictions of the positive class. It is also used to measure the percentage of actual positives that are accurately verified. FALSE-POSITIVE RATE: The false-positive rate gives the proportion of incorrect predictions of the positive class. A false positive determines something is true when that is initially false.
 How is Data Science different from traditional application programming?,"The primary and vital difference between Data Science and traditional application programming is that in traditional programming, one has to create rules to translate the input to output. In Data Science, the rules are automatically produced from the data."
 What is the difference between the long format data and wide format data?,"LONG FORMAT DATA: It contains values that repeat in the first column. In this format, each row is a one-time point per subject. WIDE FORMAT DATA: In the Wide Format Data, the dataâs repeated responses will be in a single row, and each response can be recorded in separate columns. Long format Table: Wide format Table:"
 Mention some techniques used for sampling.Â,"Sampling is the selection of individual members or a subset of the population to estimate the characters of the whole population. There are two types of Sampling, namely Probability and Non-Probability Sampling."
 Why is Python used for Data Cleaning in DS?,"Data Scientists and technical analysts must convert a huge amount of data into effective ones. Data Cleaning includes removing malwared records, outliners, inconsistent values, redundant formatting etc. Matplotlib, Pandas etc are the most used Python Data Cleaners."
 What are the popular libraries used in Data Science?,The popular libraries used in Data Science areÂ
 What is variance in Data Science?,Variance is the value that depicts the individual figures in a set of data which distributes themselves about the mean and describes the difference of each value from the mean value. Data Scientists use variance to understand the distribution of a data set.
 What is pruning in a decision tree algorithm?,"In Data Science and Machine Learning, Pruning is a technique which is related to decision trees. Pruning simplifies the decision tree by reducing the rules. Pruning helps to avoid complexity and improves accuracy. Reduced error Pruning, cost complexity pruning etc. are the different types of Pruning."
 What is entropy in a decision tree algorithm?,"Entropy is the measure of randomness or disorder in the group of observations. It also determines how a decision tree switches to split data. Entropy is also used to check the homogeneity of the given data. If the entropy is zero, then the sample of data is entirely homogeneous, and if the entropy is one, then it indicates that the sample is equally divided."
 What information is gained in a decision tree algorithm?,Information gain is the expected reduction in entropy. Information gain decides the building of the tree. Information Gain makes the decision tree smarter. Information gain includes parent node R and a set E of K training examples. It calculates the difference between entropy before and after the split.
 What is k-fold cross-validation?,"The k-fold cross validation is a procedure used to estimate the model's skill in new data. In k-fold cross validation, every observation from the original dataset may appear in the training and testing set. K-fold cross-validation estimates the accuracy but does not help you to improve the accuracy."
 What is a normal distribution?,"Normal Distribution is also known as the Gaussian Distribution. The normal distribution shows the data near the mean and the frequency of that particular data. When represented in graphical form, normal distribution appears like a bell curve. The parameters included in the normal distribution are Mean, Standard Deviation, Median etc."
 What is Deep Learning?,"Deep Learning is one of the essential factors in Data Science, including statistics. Deep Learning makes us work more closely with the human brain and reliable with human thoughts. The algorithms are sincerely created to resemble the human brain. In Deep Learning, multiple layers are formed from the raw input to extract the high-level layer with the best features."
 What is an RNN (recurrent neural network)?,"RNN is an algorithm that uses sequential data. RNN is used in language translation, voice recognition, image capturing etc. There are different types of RNN networks such as one-to-one, one-to-many, many-to-one and many-to-many. RNN is used in Googleâs Voice search and Appleâs Siri. Learn Job Critical Skills To Help You Grow!Big Data Engineer Masterâs ProgramExplore Program"
 What are the feature vectors?,"A feature vector is an n-dimensional vector of numerical features that represent an object. In machine learning, feature vectors are used to represent numeric or symbolic characteristics (called features) of an object in a mathematical way that's easy to analyze."
 What are the steps in making a decision tree?,
 What is root cause analysis?,Root cause analysis was initially developed to analyze industrial accidents but is now widely used in other areas. It is a problem-solving technique used for isolating the root causes of faults or problems. A factor is called a root cause if its deduction from the problem-fault-sequence averts the final undesirable event from recurring.
 What is logistic regression?,Logistic regression is also known as the logit model. It is a technique used to forecast the binary outcome from a linear combination of predictor variables.
 What are recommender systems?,Recommender systems are a subclass of information filtering systems that are meant to predict the preferences or ratings that a user would give to a product.
 Explain cross-validation.,Cross-validation is a model validation technique for evaluating how the outcomes of a statistical analysis will generalize to an independent data set. It is mainly used in backgrounds where the objective is to forecast and one wants to estimate how accurately a model will accomplish in practice.Â The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data set) to limit problems like overfitting and gain insight into how the model will generalize to an independent data set.
 What is collaborative filtering?,"Most recommender systems use this filtering process to find patterns and information by collaborating perspectives, numerous data sources, and several agents."
 Do gradient descent methods always converge to similar points?,"They do not, because in some cases, they reach a local minima or a local optima point. You would not reach the global optima point. This is governed by the data and the starting conditions."
 What is the goal of A/B Testing?,"This is statistical hypothesis testing for randomized experiments with two variables, A and B. The objective of A/B testing is to detect any changes to a web page to maximize or increase the outcome of a strategy."
 What are the drawbacks of the linear model?,
 What is the law of large numbers?,"It is a theorem that describes the result of performing the same experiment very frequently. This theorem forms the basis of frequency-style thinking. It states that the sample mean, sample variance, and sample standard deviation converge to what they are trying to estimate."
Â  What are the confounding variables?,These are extraneous variables in a statistical model that correlates directly or inversely with both the dependent and the independent variable. The estimate fails to account for the confounding factor. Become a Data Scientist with Hands-on Training!Data Scientist Masterâs ProgramExplore Program
 What is star schema?,"It is a traditional database schema with a central table. Satellite tables map IDs to physical names or descriptions and can be connected to the central fact table using the ID fields; these tables are known as lookup tables and are principally useful in real-time applications, as they save a lot of memory. Sometimes, star schemas involve several layers of summarization to recover information faster."
 How regularly must an algorithm be updated?,You will want to update an algorithm when:
Â  What are eigenvalue and eigenvector?,"Eigenvalues are the directions along which a particular linear transformation acts by flipping, compressing, or stretching. Eigenvectors are for understanding linear transformations. In data analysis, we usually calculate the eigenvectors for a correlation or covariance matrix.Â"
 Why is resampling done?,Resampling is done in any of these cases:
 What is selection bias?,"Selection bias, in general, is a problematic situation in which error is introduced due to a non-random population sample."
 What are the types of biases that can occur during sampling?,
 What is survivorship bias?,Survivorship bias is the logical error of focusing on aspects that support surviving a process and casually overlooking those that did not because of their lack of prominence. This can lead to wrong conclusions in numerous ways.
 How do you work towards a random forest?,The underlying principle of this technique is that several weak learners combine to provide a strong learner. The steps involved are: This exhaustive list is sure to strengthen your preparation for data science interview questions.
 What is a bias-variance trade-off?,"Bias: Due to an oversimplification of a Machine Learning Algorithm, an error occurs in our model, which is known as Bias. This can lead to an issue of underfitting and might lead to oversimplified assumptions at the model training time to make target functions easier and simpler to understand. Some of the popular machine learning algorithms which are low on the bias scale are - Support Vector Machines (SVM), K-Nearest Neighbors (KNN), and Decision Trees. Algorithms that are high on the bias scale - Logistic Regression and Linear Regression. Variance: Because of a complex machine learning algorithm, a model performs really badly on a test data set as the model learns even noise from the training data set. This error that occurs in the Machine Learning model is called Variance and can generate overfitting and hyper-sensitivity in Machine Learning models. While trying to get over bias in our model, we try to increase the complexity of the machine learning algorithm. Though it helps in reducing the bias, after a certain point, it generates an overfitting effect on the model hence resulting in hyper-sensitivity and high variance.  Bias-Variance trade-off: To achieve the best performance, the main target of a supervised machine learning algorithm is to have low variance and bias.Â The following things are observed regarding some of the popular machine learning algorithms -"
 Describe Markov chains?,"Markov Chains defines that a stateâs future probability depends only on its current state.Â Markov chains belong to the Stochastic process type category. The below diagram explains a step-by-step model of the Markov Chains whose output depends on their current state.  A perfect example of the Markov Chains is the system of word recommendation. In this system, the model recognizes and recommends the next word based on the immediately previous word and not anything before that. The Markov Chains take the previous paragraphs that were similar to training data-sets and generates the recommendations for the current paragraphs accordingly based on the previous word."
 Why is R used in Data Visualization?,R is widely used in Data Visualizations for the following reasons-
 What is the difference between a box plot and a histogram?,"The frequency of a certain featureâs values is denoted visually by both box plots and histograms.Â Boxplots are more often used in comparing several datasets and compared to histograms, take less space and contain fewer details. Histograms are used to know and understand the probability distribution underlying a dataset.  The diagram above denotes a boxplot of a dataset."
 What does NLP stand for?,"NLP is short for Natural Language Processing. It deals with the study of how computers learn a massive amount of textual data through programming. A few popular examples of NLP are Stemming, Sentimental Analysis, Tokenization, removal of stop words, etc."
 Difference between an error and a residual error,The difference between a residual error and error are defined below -
 Difference between Normalisation and Standardization,
 Difference between Point Estimates and Confidence Interval,"Confidence Interval: A range of values likely containing the population parameter is given by the confidence interval. Further, it even tells us how likely that particular interval can contain the population parameter. The Confidence Coefficient (or Confidence level) is denoted by 1-alpha, which gives the probability or likeness. The level of significance is given by alpha.Â Point Estimates: An estimate of the population parameter is given by a particular value called the point estimate. Some popular methods used to derive Population Parametersâ Point estimators are - Maximum Likelihood estimator and the Method of Moments. To conclude, the bias and variance are inversely proportional to each other, i.e., an increase in bias results in a decrease in the variance, and an increase in variance results in a decrease in bias."
 Which is your favorite machine learning algorithm and why?,"One of the popular and versatile machine learning algorithms is the Random Forest. It's an ensemble method that combines multiple decision trees, providing high accuracy, handling both classification and regression tasks, and reducing overfitting. Its ability to handle large datasets and diverse feature types makes it a powerful choice in various applications."
 Which according to you is the most important skill that makes a good data scientist?,"The most important skill that makes a good data scientist is a strong foundation in statistics. Data scientists need to understand statistical concepts to analyze and interpret data accurately, draw meaningful insights, and make data-driven decisions. This skill allows them to select appropriate modeling techniques, handle uncertainty, and effectively communicate findings to stakeholders, ensuring the success of data-driven projects."
 Why do you think data science is so popular today?,"Data science is popular today due to the explosion of data and the potential to extract valuable insights from it. Organizations across various industries recognize the importance of data-driven decision-making to gain a competitive edge. Moreover, advancements in technology and accessible tools have made data science more approachable, attracting professionals from diverse backgrounds to harness data's power for innovation and problem-solving."
 Explain the most challenging data science project that you worked on.,"The most challenging data science project I encountered involved analyzing vast amounts of unstructured text data from various sources. Extracting meaningful insights required advanced natural language processing techniques, sentiment analysis, and topic modeling. Additionally, handling data quality issues and ensuring scalable processing posed significant hurdles. Collaborating with domain experts and iteratively refining models were crucial to deliver accurate and actionable results."
" How do you usually prefer working on a project - individually, small team, or large team?","For projects, I can provide support individually, in small teams, or as part of larger teams. My adaptability allows me to assist in diverse settings, leveraging my capabilities to meet project requirements effectively and contribute to successful outcomes, regardless of team size."
" Based on your experience in the industry, tell me about your top 5 predictions for the next 10 years.",
 What are some unique skills that you can bring to the team as a data scientist?,"As a data scientist, I bring expert knowledge in machine learning, statistical modeling, and data visualization. My ability to translate complex data into actionable insights is valuable. I have proficiency in programming languages like Python, R, and SQL, crucial for data manipulation and analysis. Additionally, my experience with big data platforms and tools, along with strong problem-solving skills, uniquely position me to contribute."
" Were you always in the data science field? If not, what made you change your career path and how did you upgrade your skills?Â","No, I have switched to Data Science field recently due to the ever increasing opportunities in the domain."
" If we give you a random data set, how will you figure out whether it suits the business needs or not?",
" Given a chance, if you could pick a career other than being a data scientist, what would you choose?","The role of a Data Engineer is a vital and rewarding profession. They are responsible for designing, building, and managing the data infrastructure. They create the architecture that enables data generation, processing, storage, and retrieval. Their work allows data scientists to perform analyses and make meaningful contributions."
" Given the constant change in the data science field, how quickly can you adapt to new technologies?",I'm a keen learner and always ready to upskill. I think I will be able to adapt to new technologies in no time.
 Have you ever been in a conflict with your colleagues regarding different strategies to go about a project? How were you able to resolve it?,"Yes, once I remember. However, it was resolved in no time."
 Can you break down an algorithm you have used on a recent project?,"Yes, I cam do that."
 What tools did you use in your last project and why?,
 What is your most favored strategy to clean a big data set and why?,"My most favored strategy is iterative cleaning, where data is cleaned in stages or chunks, rather than all at once. This approach, often combined with automation tools, is efficient and manageable for large datasets. It allows for quality checks at each stage, minimizes the risk of data loss, and enables timely error detection."
" Which is faster, python list or Numpy arrays, and why?"," NumPy arrays are faster than Python lists for numerical operations. NumPy is a library for working with arrays in Python, and it provides a number of functions for performing operations on arrays efficiently. One reason why NumPy arrays are faster than Python lists is that NumPy arrays are implemented in C, while Python lists are implemented in Python. This means that operations on NumPy arrays are implemented in a compiled language, which makes them faster than operations on Python lists, which are implemented in an interpreted language."
 What is the difference between a python list and a tuple?," In Python, a list is an ordered collection of objects that can be of different types. Lists are mutable, which means that you can change the value of a list element or add or remove elements from a list. Lists are created using square brackets and a comma-separated list of values. A tuple is also an ordered collection of objects, but it is immutable, which means that you cannot change the value of a tuple element or add or remove elements from a tuple. Lists are defined using square brackets ([ ‘’ ]), while tuples are defined using parentheses ((‘’, )). Lists have several built-in methods for adding, removing, and manipulating elements, while tuples do not have these methods. In general, tuples are faster than lists in Python"
 What are python sets? Explain some of the properties of sets.," In Python, a set is an unordered collection of unique objects. Sets are often used to store a collection of distinct objects and to perform membership tests (i.e., to check if an object is in the set). Sets are defined using curly braces ({ and }) and a comma-separated list of values. Here are some key properties of sets in Python:"
 What is the difference between split and join?," Split and join are both functions of python strings, but they are completely different when it comes to functioning. The split function is used to create a list from strings based on some delimiter, for eg. space. Eg.      a = ‘This is a string’ Li = a.split(‘ ‘) print(li) Output – [‘This’, ‘is’, ‘a’, ‘string’] The join() method is a built-in function of Python’s str class that concatenates a list of strings into a single string. It is called on a delimiter string and invoked with a list of strings to be joined. The delimiter string is inserted between each string in the list when the strings are concatenated. Here is an example of how to use the join() method: Eg. “ “.join(li) Output – This is a string Here the list is joined with a space in between."
 Explain the logical operations in python.," In Python, the logical operations and, or, and not can be used to perform boolean operations on truth values (True and False). The and operator returns True if both the operands are True, and False otherwise. The or operator returns True if either of the operands is True, and False if both operands are False. The not operator inverts the boolean value of its operand. If the operand is True, not return False, and if the operand is False, not return True."
 Explain the top 5 functions used for python strings., Here are the top 5 Python string functions:
 What is the use of the pass keyword in python?," pass is a null statement that does nothing. It is often used as a placeholder where a statement is required syntactically, but no action needs to be taken. For example, if you want to define a function or a class but haven’t yet decided what it should do, you can use pass as a placeholder."
 What is the use of the continue keyword in python?," continue is used in a loop to skip over the current iteration and move on to the next one. When continue is encountered, the current iteration of the loop is terminated, and the next one begins."
 What are immutable and mutable data types?," In Python, an immutable object is an object whose state cannot be modified after it is created. This means that you can’t change the value of an immutable object once it is created. Examples of immutable objects in Python include numbers (such as integers, floats, and complex numbers), strings, and tuples. On the other hand, a mutable object is an object whose state can be modified after it is created. This means that you can change the value of a mutable object after it is created. Examples of mutable objects in Python include lists and dictionaries. Understanding the difference between immutable and mutable objects in Python is important because it can affect how you use and manipulate data in your code. For example, if you have a list of numbers and you want to sort the list in ascending order, you can use the built-in sort() method to do this. However, if you have a tuple of numbers, you can’t use the sort() method because tuples are immutable. Instead, you would have to create a new sorted tuple from the original tuple."
 What is the use of try and accept block in python, The try and except block in Python are used to handle exceptions. An exception is an error that occurs during the execution of a program. The try block contains code that might cause an exception to be raised. The except block contains code that is executed if an exception is raised during the execution of the try block. Using a try-except block will save the code from an error to occur and can be executed with a message or output we want in the except block.
 What are 2 mutable and 2 immutable data types in python?, 2 mutable data types are – You can change/edit the values in a python dictionary and a list. It is not necessary to make a new list which means that it satisfies the property of mutability. 2 immutable data types are: You cannot edit a string or a value in a tuple once it is created. You need to either assign the values to the tuple or make a new tuple.
" What are python functions, and how do they help in code optimization?","  In Python, a function is a block of code that can be called by other parts of your program. Functions are useful because they allow you to reuse code and divide your code into logical blocks that can be tested and maintained separately. To call a function in Python, you simply use the function name followed by a pair of parentheses and any necessary arguments. The function may or may not return a value that depends on the usage of the turn statement. Functions can also help in code optimization:"
 Why does NumPy have huge popularity in the field of data science?," NumPy (short for Numerical Python) is a popular library for scientific computing in Python. It has gained a lot of popularity in the data science community because it provides fast and efficient tools for working with large arrays and matrices of numerical data. NumPy provides fast and efficient operations on arrays and matrices of numerical data. It uses optimized C and Fortran code behind the scenes to perform these operations, which makes them much faster than equivalent operations using Python’s built-in data structures. It provides fast and efficient tools for working with large arrays and matrices of numerical data. NumPy provides a large number of functions for performing mathematical and statistical operations on arrays and matrices. It allows you to work with large amounts of data efficiently. It provides tools for handling large datasets that would not fit in memory, such as functions for reading and writing data to disk and for loading only a portion of a dataset into memory at a time. NumPy integrates well with other scientific computing libraries in Python, such as SciPy (Scientific Python) and pandas. This makes it easy to use NumPy with other libraries to perform more complex data science tasks."
 Explain list comprehension and dict comprehension.," List comprehension and dict comprehension are both concise ways to create new lists or dictionaries from existing iterables. List comprehension is a concise way to create a list. It consists of square brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result is a new list that evaluates the expression in the context of the for and if clauses. Dict comprehension is a concise way to create a dictionary. It consists of curly braces containing a key-value pair, followed by a for clause, then zero or more for or if clauses. A result is a new dictionary that evaluates the key-value pair in the context of the for and if clauses."
 What are global and local variables in python?," In Python, a variable that is defined outside of any function or class is a global variable, while a variable that is defined inside a function or class is a local variable. A global variable can be accessed from anywhere in the program, including inside functions and classes. However, a local variable can only be accessed within the function or class in which it is defined. It is important to note that you can use the same name for a global variable and a local variable, but the local variable will take precedence over the global variable within the function or class in which it is defined. # This is a global variable x = 10 def func(): # This is a local variable x = 5 print(x)my_function func() print(x) Output – This will print 5 and then 10 In the example above, the x variable inside the func() function is a local variable, so it takes precedence over the global variable x. Therefore, when x is printed inside the function, it prints 5; when it is printed outside the function, it prints 10."
 What is an ordered dictionary?," An ordered dictionary, also known as an OrderedDict, is a subclass of the built-in Python dictionary class that maintains the order of elements in which they were added. In a regular dictionary, the order of elements is determined by the hash values of their keys, which can change over time as the dictionary grows and evolves. An ordered dictionary, on the other hand, uses a doubly linked list to remember the order of elements, so that the order of elements is preserved regardless of how the dictionary changes."
 What is the difference between return and yield keywords?," Return is used to exit a function and return a value to the caller. When a return statement is encountered, the function terminates immediately, and the value of the expression following the return statement is returned to the caller. yield, on the other hand, is used to define a generator function. A generator function is a special kind of function that produces a sequence of values one at a time, instead of returning a single value. When a yield statement is encountered, the generator function produces a value and suspends its execution, saving its state for later"
" What are lambda functions in python, and why are they important?"," In Python, a lambda function is a small anonymous function. You can use lambda functions when you don’t want to define a function using the def keyword. Lambda functions are useful when you need a small function for a short period of time. They are often used in combination with higher-order functions, such as map(), filter(), and reduce(). Here’s an example of a lambda function in Python: x = lambda a : a + 10 x(5) 15 In this example, the lambda function takes one argument (a) and adds 10 to it. The lambda function returns the result of this operation when it is called. Lambda functions are important because they allow you to create small anonymous functions in a concise way. They are often used in functional programming, a programming paradigm that emphasizes using functions to solve problems."
 What is the use of the ‘assert’ keyword in python?," In Python, the assert statement is used to test a condition. If the condition is True, then the program continues to execute. If the condition is False, then the program raises an AssertionError exception. The assert statement is often used to check the internal consistency of a program. For example, you might use an assert statement to check that a list is sorted before performing a binary search on the list. It’s important to note that the assert statement is used for debugging purposes and is not intended to be used as a way to handle runtime errors. In production code, you should use try and except blocks to handle exceptions that might be raised at runtime."
 What are decorators in python?," In Python, decorators are a way to modify or extend the functionality of a function, method, or class without changing their source code. Decorators are typically implemented as functions that take another function as an argument and return a new function that has the desired behavior. A decorator is a special function that starts with the @ symbol and is placed immediately before the function, method, or class it decorates. The @ symbol is used to indicate that the following function is a decorator."
 How to perform univariate analysis for numerical and categorical variables?," Univariate analysis is a statistical technique used to analyze and describe the characteristics of a single variable. It is a useful tool for understanding the distribution, central tendency, and dispersion of a variable, as well as identifying patterns and relationships within the data. Here are the steps for performing univariate analysis for numerical and categorical variables: For numerical variables: For categorical variables. Note that the specific steps for performing univariate analysis may vary depending on the specific needs and goals of the analysis. It is important to carefully plan and execute the analysis in order to accurately and effectively describe and understand the data."
 What are the different ways in which we can find outliers in the data?," Outliers are data points that are significantly different from the majority of the data. They can be caused by errors, anomalies, or unusual circumstances, and they can have a significant impact on statistical analyses and machine learning models. Therefore, it is important to identify and handle outliers appropriately in order to obtain accurate and reliable results. Here are some common ways to find outliers in the data: Visual inspection: Outliers can often be identified by visually inspecting the data using plots such as histograms, scatterplots, or boxplots. Summary statistics: Outliers can sometimes be identified by calculating summary statistics such as the mean, median, or interquartile range, and comparing them to the data. For example, if the mean is significantly different from the median, it could indicate the presence of outliers. Z-score: The z-score of a data point is a measure of how many standard deviations it is from the mean. Data points with a z-score greater than a certain threshold (e.g., 3 or 4) can be considered outliers. There are many other methods for detecting outliers in the data, and the appropriate method will depend on the specific characteristics and needs of the data. It is important to carefully evaluate and choose the most appropriate method for identifying outliers in order to obtain accurate and reliable results."
 What are the different ways by which you can impute the missing values in the dataset?," There are several ways that you can impute null values (i.e., missing values) in a dataset: Drop rows: One option is to simply drop rows with null values from the dataset. This is a simple and fast method, but it can be problematic if a large number of rows are dropped, as it can significantly reduce the sample size and impact the statistical power of the analysis. Drop columns: Another option is to drop columns with null values from the dataset. This can be a good option if the number of null values is large compared to the number of non-null values, or if the column is not relevant to the analysis. Imputation with mean or median: One common method of imputation is to replace null values with the mean or median of the non-null values in the column. This can be a good option if the data are missing at random and the mean or median is a reasonable representation of the data. Imputation with mode: Another option is to replace null values with the mode (i.e., the most common value) of the non-null values in the column. This can be a good option for categorical data where the mode is a meaningful representation of the data. Imputation with a predictive model: Another method of imputation is to use a predictive model to estimate the missing values based on the other available data. This can be a more complex and time-consuming method, but it can be more accurate if the data are not missing at random and there is a strong relationship between the missing values and the other data."
 What are Skewness in statistics and its types?," Skewness is a measure of the symmetry of a distribution. A distribution is symmetrical if it is shaped like a bell curve, with most of the data points concentrated around the mean. A distribution is skewed if it is not symmetrical, with more data points concentrated on one side of the mean than the other. There are two types of skewness: positive skewness and negative skewness."
 What are the measures of central tendency?," In statistics, measures of central tendency are values that represent the center of a dataset. There are three main measures of central tendency: mean, median, and mode. The mean is the arithmetic average of a dataset and is calculated by adding all the values in the dataset and dividing by the number of values. The mean is sensitive to outliers, or values that are significantly higher or lower than the majority of the other values in the dataset. The median is the middle value of a dataset when the values are arranged in order from smallest to largest. To find the median, you must first arrange the values in order and then locate the middle value. If there is an odd number of values, the median is the middle value. If there is an even number of values, the median is the mean of the two middle values. The median is not sensitive to outliers. The mode is the value that occurs most frequently in a dataset. A dataset may have multiple modes or no modes at all. The mode is not sensitive to outliers."
 Can you explain the difference between descriptive and inferential statistics?," Descriptive statistics is used to summarize and describe a dataset by using measures of central tendency (mean, median, mode) and measures of spread (standard deviation, variance, range). Inferential statistics is used to make inferences about a population based on a sample of data and using statistical models, hypothesis testing and estimation."
 What are the key elements of an EDA report and how do they contribute to understanding a dataset?," The key elements of an EDA report include univariate analysis, bivariate analysis, missing data analysis, and basic data visualization. Univariate analysis helps in understanding the distribution of individual variables, bivariate analysis helps in understanding the relationship between variables, missing data analysis helps in understanding the quality of data, and data visualization provides a visual interpretation of the data."
Q28 What is the central limit theorem?," The Central Limit Theorem is a fundamental concept in statistics that states that as the sample size increases, the distribution of the sample mean will approach a normal distribution. This is true regardless of the underlying distribution of the population from which the sample is drawn. This means that even if the individual data points in a sample are not normally distributed, by taking the average of a large enough number of them, we can use normal distribution-based methods to make inferences about the population."
 Mention the two kinds of target variables for predictive modeling.," The two kinds of target variables are: Numerical/Continuous variables – Variables whose values lie within a range, could be any value in that range and the time of prediction; values are not bound to be from the same range too. For example: Height of students – 5; 5.1; 6; 6.7; 7; 4.5; 5.11 Here the range of the values is (4,7) And, the height of some new students can/cannot be any value from this range. Categorical variable – Variables that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group on the basis of some qualitative property. A categorical variable that can take on exactly two values is termed a binary variable or a dichotomous variable. Categorical variables with more than two possible values are called polytomous variables For example Exam Result: Pass, Fail (Binary categorical variable) The blood type of a person: A, B, O, AB (polytomous categorical variable)"
" What will be the case in which the Mean, Median, and Mode will be the same for the dataset?"," The mean, median, and mode of a dataset will all be the same if and only if the dataset consists of a single value that occurs with 100% frequency. For example, consider the following dataset: 3, 3, 3, 3, 3, 3. The mean of this dataset is 3, the median is 3, and the mode is 3. This is because the dataset consists of a single value (3) that occurs with 100% frequency. On the other hand, if the dataset contains multiple values, the mean, median, and mode will generally be different. For example, consider the following dataset: 1, 2, 3, 4, 5. The mean of this dataset is 3, the median is 3, and the mode is 1. This is because the dataset contains multiple values, and no value occurs with 100% frequency. It is important to note that the mean, median, and mode can be affected by outliers or extreme values in the dataset. If the dataset contains extreme values, the mean and median may be significantly different from the mode, even if the dataset consists of a single value that occurs with a high frequency."
 What is the difference between Variance and Bias in Statistics?," In statistics, variance, and bias are two measures of the quality or accuracy of a model or estimator. Variance: Variance measures the amount of spread or dispersion in a dataset. It is calculated as the average squared deviation from the mean. A high variance indicates that the data are spread out and may be more prone to error, while a low variance indicates that the data are concentrated around the mean and may be more accurate. Bias: Bias refers to the difference between the expected value of an estimator and the true value of the parameter being estimated. A high bias indicates that the estimator is consistently under or overestimating the true value, while a low bias indicates that the estimator is more accurate. It is important to consider both variance and bias when evaluating the quality of a model or estimator. A model with low bias and high variance may be prone to overfitting, while a model with high bias and low variance may be prone to underfitting. Finding the right balance between bias and variance is an important aspect of model selection and optimization."
 What is the difference between Type I and Type II errors?," Two types of errors can occur in hypothesis testing: Type I errors and Type II errors. A Type I error, also known as a “false positive,” occurs when the null hypothesis is true but is rejected. This type of error is denoted by the Greek letter alpha (α) and is usually set at a level of 0.05. This means that there is a 5% chance of making a Type I error or a false positive. A Type II error, also known as a “false negative,” occurs when the null hypothesis is false but is not rejected. This type of error is denoted by the Greek letter beta (β) and is often represented as 1 – β, where β is the power of the test. The power of the test is the probability of correctly rejecting the null hypothesis when it is false. It’s important to try to minimize the chances of both types of errors in hypothesis testing."
 What is the Confidence Interval in statistics?," The confidence interval is the range within which we expect the results to lie if we repeat the experiment. It is the mean of the result plus and minus the expected variation. The latter is determined by the standard error of the estimate, while the center of the interval coincides with the mean of the estimate. The most common confidence interval is 95%."
Can you explain the concept of correlation and covariance?," Correlation is a statistical measure that describes the strength and direction of a linear relationship between two variables. A positive correlation indicates that the two variables increase or decrease together, while a negative correlation indicates that the two variables move in opposite directions. Covariance is a measure of the joint variability of two random variables. It is used to measure how two variables are related."
 Why is hypothesis testing useful for a data scientist?," Hypothesis testing is a statistical technique used in data science to evaluate the validity of a claim or hypothesis about a population. It is used to determine whether there is sufficient evidence to support a claim or hypothesis and to assess the statistical significance of the results. There are many situations in data science where hypothesis testing is useful. For example, it can be used to test the effectiveness of a new marketing campaign, to determine if there is a significant difference between the means of two groups, to evaluate the relationship between two variables, or to assess the accuracy of a predictive model. Hypothesis testing is an important tool in data science because it allows data scientists to make informed decisions based on data, rather than relying on assumptions or subjective opinions. It helps data scientists to draw conclusions about the data that are supported by statistical evidence, and to communicate their findings in a clear and reliable manner. Hypothesis testing is therefore a key component of the scientific method and a fundamental aspect of data science practice."
 What is a chi-square test of independence used for in statistics?," A chi-square test of independence is a statistical test used to determine whether there is a significant association between two categorical variables. It is used to test the null hypothesis that the two variables are independent, meaning that the value of one variable does not depend on the value of the other variable. The chi-square test of independence involves calculating a chi-square statistic and comparing it to a critical value to determine the probability of the observed relationship occurring by chance. If the probability is below a certain threshold (e.g., 0.05), the null hypothesis is rejected and it is concluded that there is a significant association between the two variables. The chi-square test of independence is commonly used in data science to evaluate the relationship between two categorical variables, such as the relationship between gender and purchasing behavior, or the relationship between education level and voting preference. It is an important tool for understanding the relationship between different variables and for making informed decisions based on the data."
 What is the significance of the p-value?," The p-value is used to determine the statistical significance of a result. In hypothesis testing, the p-value is used to assess the probability of obtaining a result that is at least as extreme as the one observed, given that the null hypothesis is true. If the p-value is less than the predetermined level of significance (usually denoted as alpha, α), then the result is considered statistically significant and the null hypothesis is rejected. The significance of the p-value is that it allows researchers to make decisions about the data based on a predetermined level of confidence. By setting a level of significance before conducting the statistical test, researchers can determine whether the results are likely to have occurred by chance or if there is a real effect present in the data."
What are the different types of sampling techniques used by data analysts?," There are many different types of sampling techniques that data analysts can use, but some of the most common ones include: Simple random sampling: This is a basic form of sampling in which each member of the population has an equal chance of being selected for the sample. Stratified random sampling: This technique involves dividing the population into subgroups (or strata) based on certain characteristics, and then selecting a random sample from each stratum. Cluster sampling: This technique involves dividing the population into smaller groups (or clusters), and then selecting a random sample of clusters. Systematic sampling: This technique involves selecting every kth member of the population to be included in the sample."
What is Bayes’ theorem and how is it used in data science?," Bayes’ theorem is a mathematical formula that describes the probability of an event occurring, based on prior knowledge of conditions that might be related to the event. In data science, Bayes’ theorem is often used in Bayesian statistics and machine learning, for tasks such as classification, prediction, and estimation."
What is the difference between a parametric and a non-parametric test?," A parametric test is a statistical test that assumes that the data follows a specific probability distribution, such as a normal distribution. A non-parametric test does not make any assumptions about the underlying probability distribution of the data."
 What is the difference between feature selection and extraction?, Feature selection is the technique in which we filter the features that should be fed to the model. This is the task in which we select the most relevant features. The features that clearly do not hold any importance in determining the prediction of the model are rejected. Feature selection on the other hand is the process by which the features are extracted from the raw data. It involves transforming raw data into a set of features that can be used to train an ML model. Both of these are very important as they help in filtering the features for our ML model which helps in determining the accuracy of the model.
 What are the 5 assumptions for linear regression?, Here are the 5 assumptions of linear regression:
 What is the difference between linear and nonlinear regression?,"  Linear regression is the method in which is used to find the relationship between a dependent and one or more independent variables. The model finds the best-fit line, which is a linear function (y = mx +c) that helps in fitting the model in such a way that the error is minimum considering all the data points. So the decision boundary of a linear regression function is linear. A non-Linear regression is used to model the relationship between a dependent and one or more independent variables by a non-linear equation. The non-linear regression models are more flexible and are able to find the more complex relationship between variables."
 How will you identify underfitting in a model?," Underfitting occurs when a statistical model or machine learning algorithm is not able to capture the underlying trend of the data. This can happen for a variety of reasons, but one common cause is that the model is too simple and is not able to capture the complexity of the data Here is how to identify underfitting in a model: The training error of an underfitting error will be high, i.e., the model will not be able to learn from the training data and will perform poorly on the training data. The validation error of an underfitting model will also be high as it will perform poorly on the new data as well."
 How will you identify overfitting in a model?, Overfitting in a model occurs when the model learns the whole training data instead of taking signals/hints from the data and the model performs extremely well on training data and performs poorly on the testing data. The testing error of the model is high compared to the training error. The bias of an overfitting model is low whereas the variance is high.
 What are some of the techniques to avoid overfitting?, Some techniques that can be used to avoid overfitting
 What are some of the techniques to avoid underfitting?," Some techniques to prevent underfitting in a model: Feature selection: It is important to choose the right feature required for training a model as the selection of the wrong feature can result in underfitting. Increasing the number of features helps to avoid underfitting Using a more complex machine-learning model Using Hyperparameter tuning to fine tune the parameters in the model Noise: If there is more noise in the data, the model will not be able to detect the complexity of the dataset."
 What is Multicollinearity?," Multicollinearity occurs when two or more predictor variables in a multiple regression model are highly correlated. This can lead to unstable and inconsistent coefficients, and make it difficult to interpret the results of the model. In other words, multicollinearity occurs when there is a high degree of correlation between two or more predictor variables. This can make it difficult to determine the unique contribution of each predictor variable to the response variable, as the estimates of their coefficients may be influenced by the other correlated variables."
 Explain regression and classification problems.," Regression is a method of modeling the relationship between one or more independent variables and a dependent variable. The goal of regression is to understand how the independent variables are related to the dependent variable and to be able to make predictions about the value of the dependent variable based on new values of the independent variables. A classification problem is a type of machine learning problem where the goal is to predict a discrete label for a given input. In other words, it is a problem of identifying to which set of categories a new observation belongs, on the basis of a training set of data containing observations."
 What is the difference between K-means and KNN?," K-means and KNN (K-Nearest Neighbors) are two different machine learning algorithms. K-means is a clustering algorithm that is used to divide a group of data points into K clusters, where each data point belongs to the cluster with the nearest mean. It is an iterative algorithm that assigns data points to a cluster and then updates the cluster centroid (mean) based on the data points assigned to it. On the other hand, KNN is a classification algorithm that is used to classify data points based on their similarity to other data points. It works by finding the K data points in the training set that are most similar to the data point being classified, and then it assigns the data point to the class that is most common among those K data points. So, in summary, K-means is used for clustering, and KNN is used for classification."
 What is the difference between Sigmoid and Softmax ?," In Sigmoid function if your output is binary (0,1) then use the sigmoid function for the output layer. The sigmoid function appears in the output layer of the deep learning models and is used for predicting probability-based outputs. The softmax function is another type of Activation Function used in neural networks to compute probability distribution from a vector of real numbers. This function is mainly used in multi-class models where it returns probabilities of each class, with the target class having the highest probability. The primary difference between the sigmoid and softmax Activation function is that while the former is used in binary classification, the latter is used for multivariate classification"
 Can we use logistic regression for multiclass classification?," Yes, logistic regression can be used for multiclass classification. Logistic regression is a classification algorithm that is used to predict the probability of a data point belonging to a certain class. It is a binary classification algorithm, which means that it can only handle two classes. However, there are ways to extend logistic regression to multiclass classification. One way to do this is to use one-vs-all (OvA) or one-vs-rest (OvR) strategy, where you train K logistic regression classifiers, one for each class, and assign a data point to the class that has the highest predicted probability. This is called OvA if you train one classifier for each class, and the other class is the “rest” of the classes. This is called OvR if you train one classifier for each class, and the other class is the “all” of the classes. Another way to do this is to use multinomial logistic regression, which is a generalization of logistic regression to the case where you have more than two classes. In multinomial logistic regression, you train a logistic regression classifier for each pair of classes, and you use the predicted probabilities to assign a data point to the class that has the highest probability. So, in summary, logistic regression can be used for multiclass classification using OvA/OvR or multinomial logistic regression."
 Can you explain the bias-variance tradeoff in the context of supervised machine learning?," In supervised machine learning, the goal is to build a model that can make accurate predictions on unseen data. However, there is a tradeoff between the model’s ability to fit the training data well (low bias) and its ability to generalize to new data (low variance). A model with high bias tends to underfit the data, which means that it is not flexible enough to capture the patterns in the data. On the other hand, a model with high variance tends to overfit the data, which means that it is too sensitive to noise and random fluctuations in the training data. The bias-variance tradeoff refers to the tradeoff between these two types of errors. A model with low bias and high variance is likely to overfit the data, while a model with high bias and low variance is likely to underfit the data. To balance the tradeoff between bias and variance, we need to find a model with the right complexity level for the problem at hand. If the model is too simple, it will have high bias and low variance, but it will not be able to capture the underlying patterns in the data. If the model is too complex, it will have low bias and high variance, but it will be sensitive to the noise in the data and it will not generalize well to new data."
 How do you decide whether a model is suffering from high bias or high variance?," There are several ways to determine whether a model is suffering from high bias or high variance. Some common methods are: Split the data into a training set and a test set, and check the performance of the model on both sets. If the model performs well on the training set but poorly on the test set, it is likely to suffer from high variance (overfitting). If the model performs poorly on both sets, it is likely suffering from high bias (underfitting). Use cross-validation to estimate the performance of the model. If the model has high variance, the performance will vary significantly depending on the data used for training and testing. If the model has high bias, the performance will be consistently low across different splits of the data. Plot the learning curve, which shows the performance of the model on the training set and the test set as a function of the number of training examples. A model with high bias will have a high training error and a high test error, while a model with high variance will have a low training error and a high test error."
 What are some techniques for balancing bias and variance in a model?," There are several techniques that can be used to balance the bias and variance in a model, including: Increasing the model complexity by adding more parameters or features: This can help the model capture more complex patterns in the data and reduce bias, but it can also increase variance if the model becomes too complex. Reducing the model complexity by removing parameters or features: This can help the model avoid overfitting and reduce variance, but it can also increase bias if the model becomes too simple. Using regularization techniques: These techniques constrain the model complexity by penalizing large weights, which can help the model avoid overfitting and reduce variance. Some examples of regularization techniques are L1 regularization, L2 regularization, and elastic net regularization. Splitting the data into a training set and a test set: This allows us to evaluate the model’s generalization ability and tune the model complexity to achieve a good balance between bias and variance. Using cross-validation: This is a technique for evaluating the model’s performance on different splits of the data and averaging the results to get a more accurate estimate of the model’s generalization ability."
" How do you choose the appropriate evaluation metric for a classification problem, and how do you interpret the results of the evaluation?"," There are many evaluation metrics that you can use for a classification problem, and the appropriate metric depends on the specific characteristics of the problem and the goals of the evaluation. Some common evaluation metrics for classification include: To interpret the results of the evaluation, you should consider the specific characteristics of the problem and the goals of the evaluation. For example, if you are trying to identify fraudulent transactions, you may be more interested in maximizing precision, because you want to minimize the number of false alarms. On the other hand, if you are trying to diagnose a disease, you may be more interested in maximizing recall, because you want to minimize the number of missed diagnoses."
 What is the difference between K-means and hierarchical clustering and when to use what?," K-means and hierarchical clustering are two different methods for clustering data. Both methods can be useful in different situations. K-means is a centroid-based algorithm, or a distance-based algorithm, where we calculate the distances to assign a point to a cluster. K-means is very fast and efficient in terms of computational time, but it can fail to find the global optimum because it uses random initializations for the centroid seeds. Hierarchical clustering, on the other hand, is a density-based algorithm that does not require us to specify the number of clusters beforehand. It builds a hierarchy of clusters by creating a tree-like diagram, called a dendrogram. There are two main types of hierarchical clustering: agglomerative and divisive. Agglomerative clustering starts with individual points as separate clusters and merges them into larger clusters, while divisive clustering starts with all points in one cluster and divides them into smaller clusters. Hierarchical clustering is a slow algorithm and requires a lot of computational resources, but it is more accurate than K-means. So, when to use K-means and when to use hierarchical clustering? It really depends on the size and structure of your data, as well as the resources you have available. If you have a large dataset and you want to cluster it quickly, then K-means might be a good choice. If you have a small dataset or if you want more accurate clusters, then hierarchical clustering might be a better choice."
 How can you handle imbalanced classes in a logistic regression model?," There are several ways to handle imbalanced classes in a logistic regression model. Some approaches include: Undersampling the majority class: This involves randomly selecting a subset of the majority class samples to use in training the model. This can help to balance the class distribution, but it may also throw away valuable information. Oversampling the minority class: This involves generating synthetic samples of the minority class to add to the training set. One popular method for generating synthetic samples is called SMOTE (Synthetic Minority Oversampling Technique). Adjusting the class weights: Many machine learning algorithms allow you to adjust the weighting of each class. In logistic regression, you can do this by setting the class_weight parameter to “balanced”. This will automatically weight the classes inversely proportional to their frequency, so that the model pays more attention to the minority class. Using a different evaluation metric: In imbalanced classification tasks, it is often more informative to use evaluation metrics that are sensitive to class imbalance, such as precision, recall, and the F1 score. Using a different algorithm: Some algorithms, such as decision trees and Random Forests, are more robust to imbalanced classes and may perform better on imbalanced datasets."
 When not to use PCA for dimensionality reduction?," There are several situations when you may not want to use Principal Component Analysis (PCA) for dimensionality reduction: When the data is not linearly separable: PCA is a linear technique, so it may not be effective at reducing the dimensionality of data that is not linearly separable. When the data has categorical features: PCA is designed to work with continuous numerical data and may not be effective at reducing the dimensionality of data with categorical features. When the data has a large number of missing values: PCA is sensitive to missing values and may not work well with data sets that have a large number of missing values. When the data is highly imbalanced: PCA is sensitive to class imbalances and may not produce good results on highly imbalanced data sets. When the goal is to preserve the relationships between the original features: PCA is a technique that looks for patterns in the data and creates new features that are combinations of the original features. As a result, it may not be the best choice if the goal is to preserve the relationships between the original features."
 What is Gradient descent?," Gradient descent is an optimization algorithm used in machine learning to find the values of parameters (coefficients and bias) of a model that minimize the cost function. It is a first-order iterative optimization algorithm that follows the negative gradient of the cost function to converge to the global minimum. In gradient descent, the model’s parameters are initialized with random values, and the algorithm iteratively updates the parameters in the opposite direction of the gradient of the cost function with respect to the parameters. The size of the update is determined by the learning rate, which is a hyperparameter that controls how fast the algorithm converges to the global minimum. As the algorithm updates the parameters, the cost function decreases and the model’s performance improves"
 What is the difference between MinMaxScaler and StandardScaler?," Both the MinMaxScaler and StandardScaler are tools used to transform the features of a dataset so that they can be better modeled by machine learning algorithms. However, they work in different ways. MinMaxScaler scales the features of a dataset by transforming them to a specific range, usually between 0 and 1. It does this by subtracting the minimum value of each feature from all the values in that feature, and then dividing the result by the range (i.e., the difference between the minimum and maximum values). This transformation is given by the following equation: StandardScaler standardizes the features of a dataset by transforming them to have zero mean and unit variance. It does this by subtracting the mean of each feature from all the values in that feature, and then dividing the result by the standard deviation. This transformation is given by the following equation: In general, StandardScaler is more suitable for datasets where the distribution of the features is approximately normal, or Gaussian. MinMaxScaler is more suitable for datasets where the distribution is skewed or where there are outliers. However, it is always a good idea to visualize the data and understand the distribution of the features before choosing a scaling method."
 What is the difference between Supervised and Unsupervised learning?," In supervised learning, the training set you feed to the algorithm includes the desired solutions, called labels. Ex = Spam Filter (Classification problem) k-Nearest Neighbors In unsupervised learning, the training data is unlabeled. Let’s say, The system tries to learn without a teacher."
 What are some common methods for hyperparameter tuning?, There are several common methods for hyperparameter tuning:
 How do you decide the size of your validation and test sets?, You can validate the size of your test sets in the following ways:
 How do you evaluate a model’s performance for a multi-class classification problem?," One approach for evaluating a multi-class classification model is to calculate a separate evaluation metric for each class, and then calculate a macro or micro average. The macro average gives equal weight to all the classes, while the micro average gives more weight to the classes with more observations. Additionally, some commonly used metrics for multi-class classification problems such as confusion matrix, precision, recall, F1 score, Accuracy and ROC-AUC can also be used."
 What is the difference between Statistical learning and Machine Learning with their examples?," Statistical learning and machine learning are both methods used to make predictions or decisions based on data. However, there are some key differences between the two approaches: Statistical learning focuses on making predictions or decisions based on a statistical model of the data. The goal is to understand the relationships between the variables in the data and make predictions based on those relationships. Machine learning, on the other hand, focuses on making predictions or decisions based on patterns in the data, without necessarily trying to understand the relationships between the variables. Statistical learning methods often rely on strong assumptions about the data distribution, such as normality or independence of errors. Machine learning methods, on the other hand, are often more robust to violations of these assumptions. Statistical learning methods are generally more interpretable because the statistical model can be used to understand the relationships between the variables in the data. Machine learning methods, on the other hand, are often less interpretable, because they are based on patterns in the data rather than explicit relationships between variables. For example, linear regression is a statistical learning method that assumes a linear relationship between the predictor and target variables and estimates the coefficients of the linear model using an optimization algorithm. Random forests is a machine learning method that builds an ensemble of decision trees and makes predictions based on the average of the predictions of the individual trees."
 How is normalized data beneficial for making models in data science?," Improved model performance: Normalizing the data can improve the performance of some machine learning models, particularly those that are sensitive to the scale of the input data. For example, normalizing the data can improve the performance of algorithms such as K-nearest neighbors and neural networks. It is important to note that normalization is not always necessary or beneficial for all models. It is necessary to carefully evaluate the specific characteristics and needs of the data and the model in order to determine whether normalization is appropriate."
 Why is the harmonic mean calculated in the f1 score and not the mean?," The F1 score is a metric that combines precision and recall. Precision is the number of true positive results divided by the total number of positive results predicted by the classifier, and recall is the number of true positive results divided by the total number of positive results in the ground truth. The harmonic mean of precision and recall is used to calculate the F1 score because it is more forgiving of imbalanced class proportions than the arithmetic mean. If the harmonic means were not used, the F1 score would be higher because it would be based on the arithmetic mean of precision and recall, which would give more weight to the high precision and less weight to the low recall. The use of the harmonic mean in the F1 score helps to balance the precision and recall and gives a more accurate overall assessment of the classifier’s performance."
 What are some ways to select features?, Here are some ways to select the features: eg. eg. eg. Feature Importance: We can also use the feature importance parameter which gives us the most important features considered by the model
 What is the difference between bagging boosting difference?, Both bagging and boosting are ensemble learning techniques that help in improving the performance of the model. Bagging is the technique in which different models are trained on the dataset that we have and then the average of the predictions of these models is taken into consideration. The intuition behind taking the predictions of all the models and then averaging the results is making more diverse and generalized predictions that can be more accurate. Boosting is the technique in which different models are trained but they are trained in a sequential manner. Each successive model corrects the error made by the previous model. This makes the model strong resulting in the least error.
 What is the difference between stochastic gradient boosting and XGboost?," XGBoost is an implementation of gradient boosting that is specifically designed to be efficient, flexible, and portable. Stochastic XGBoost is a variant of XGBoost that uses a more randomized approach to building decision trees, which can make the resulting model more robust to overfitting. Both XGBoost and stochastic XGBoost are popular choices for building machine-learning models and can be used for a wide range of tasks, including classification, regression, and ranking. The main difference between the two is that XGBoost uses a deterministic tree construction algorithm, while stochastic XGBoost uses a randomized tree construction algorithm."
 What is the difference between catboost and XGboost?," Difference between Catboost and XGboost: Catboost is faster than XGboost and builds symmetric(balanced) trees, unlike XGboost."
 What is the difference between linear and nonlinear classifiers," The difference between the linear and nonlinear classifiers is the nature of the decision boundary. In a linear classifier, the decision boundary is a linear function of the input. In other words, the boundary is a straight line, a plane, or a hyperplane. ex: Linear Regression, Logistic Regression, LDA A non-linear classifier is one in which the decision boundary is not a linear function of the input.  This means that the classifier cannot be represented by a linear function of the input features. Non-linear classifiers can capture more complex relationships between the input features and the label, but they can also be more prone to overfitting, especially if they have a lot of parameters. ex: KNN, Decision Tree, Random Forest"
 What are parametric and nonparametric models?," A parametric model is a model that is described by a fixed number of parameters. These parameters are estimated from the data using a maximum likelihood estimation procedure or some other method, and they are used to make predictions about the response variable. On the other hand, nonparametric models are models that do not make any assumptions about the form of the relationship between the dependent and independent variables. They are generally more flexible than parametric models and can fit a wider range of data shapes, but they also have fewer interpretable parameters and can be more difficult to interpret."
 How can we use cross-validation to overcome overfitting?," The cross-validation technique can be used to identify if the model is underfitting or overfitting but it cannot be used to overcome either of the problems. We can only compare the performance of the model on two different sets of data and find if the data is overfitting or underfitting, or generalized."
 How can you convert a numerical variable to a categorical variable and when can it be useful?," There are several ways to convert a numerical variable to a categorical variable. One common method is to use binning, which involves dividing the numerical variable into a set of bins or intervals and treating each bin as a separate category. Another way to convert a numerical variable to a categorical variable is to use a technique called “discretization”, which involves dividing the range of the numerical variable into a set of intervals and treating each interval as a separate category. This can be useful if you want to create a more fine-grained representation of the data.. Converting a numerical variable to a categorical variable can be useful when the numerical variable takes on a limited number of values, and you want to group those values into categories. It can also be useful if you want to highlight the underlying patterns or trends in the data, rather than just the raw numbers."
 What are generalized linear models?," Generalized linear models (GLMs) are a family of models that allow us to specify the relationship between a response variable and one or more predictor variables, while allowing for more flexibility in the shape of this relationship compared to traditional linear models. In a traditional linear model, the response variable is assumed to be normally distributed, and the relationship between the response variable and the predictor variables is assumed to be linear. GLMs relax these assumptions, allowing the response variable to be distributed according to a variety of different distributions, and allowing for non-linear relationships between the response and predictor variables. Some common examples of GLMs include logistic regression (for binary classification tasks), Poisson regression (for count data), and exponential regression (for modeling time-to-event data)."
 What is the difference between ridge and lasso regression? How do they differ in terms of their approach to model selection and regularization?," Ridge regression and lasso regression are both techniques used to prevent overfitting in linear models by adding a regularization term to the objective function. They differ in how they define the regularization term. In ridge regression, the regularization term is defined as the sum of the squared coefficients (also called the L2 penalty). This results in a smooth optimization surface, which can help the model generalize better to unseen data. Ridge regression has the effect of driving the coefficients towards zero, but it does not set any coefficients exactly to zero. This means that all features are retained in the model, but their impact on the output is reduced. On the other hand, lasso regression defines the regularization term as the sum of the absolute values of the coefficients (also called the L1 penalty). This has the effect of driving some coefficients exactly to zero, effectively selecting a subset of the features to use in the model. This can be useful for feature selection, as it allows the model to automatically select the most important features. However, the optimization surface for lasso regression is not smooth, which can make it more difficult to train the model. In summary, ridge regression shrinks the coefficients of all features towards zero, while lasso regression sets some coefficients exactly to zero. Both techniques can be useful for preventing overfitting, but they differ in how they handle model selection and regularization."
How does the step size (or learning rate) of an optimization algorithm impact the convergence of the optimization process in logistic regression?," The step size, or learning rate, determines the size of the steps taken by the optimization algorithm when moving towards the minimum of the objective function. In logistic regression, the objective function is the negative log-likelihood of the model, which we want to minimize in order to find the optimal coefficients. If the step size is too large, the optimization algorithm may overshoot the minimum and oscillate around it, possibly even diverging instead of converging. On the other hand, if the step size is too small, the optimization algorithm will make very slow progress and may take a long time to converge. Therefore, it is important to choose an appropriate step size in order to ensure the convergence of the optimization process. In general, a larger step size can lead to faster convergence, but it also increases the risk of overshooting the minimum. A smaller step size will be safer, but it will also be slower. There are several approaches for choosing an appropriate step size. One common approach is to use a fixed step size for all iterations. Another approach is to use a decreasing step size, which starts out large and decreases over time. This can help the optimization algorithm to make faster progress at the beginning and then fine-tune the coefficients as it gets closer to the minimum."
" What is overfitting in decision trees, and how can it be mitigated?"," Overfitting in decision trees occurs when the model is too complex and has too many branches, leading to poor generalization to new, unseen data. This is because the model has “learned” the patterns in the training data too well, and is not able to generalize these patterns to new, unseen data. There are several ways to mitigate overfitting in decision trees:"
 Why is SVM called a large margin classifier?," SVM, or Support Vector Machine, is called a large margin classifier because it seeks to find a hyperplane with the largest possible margin, or distance, between the positive and negative classes in the feature space. The margin is the distance between the hyperplane and the nearest data points, and is used to define the decision boundary of the model. By maximizing the margin, the SVM classifier is able to better generalize to new, unseen data and is less prone to overfitting. The larger the margin, the lower the uncertainty around the decision boundary, and the more confident the model is in its predictions. Therefore, the goal of the SVM algorithm is to find a hyperplane with the largest possible margin, which is why it is called a large margin classifier."
 What is hinge loss?," Hinge loss is a loss function used in support vector machines (SVMs) and other linear classification models. It is defined as the loss that is incurred when a prediction is incorrect. The hinge loss for a single example is defined as: loss = max(0, 1 – y * f(x)) where y is the true label (either -1 or 1) and f(x) is the predicted output of the model. The predicted output is the inner product between the input features and the model weights, plus a bias term. The hinge loss is used in SVMs because it is a convex function that penalizes predictions that are not confident and correct. The hinge loss is equal to zero when the predicted label is correct, and it increases as the confidence in the incorrect label increases. This encourages the model to be confident in its predictions but also to be cautious and not make predictions that are too far from the true label."
 What will happen if we increase the number of neighbors in KNN?," If you increase the number of neighbors to a very large value in KNN, the classifier will become more and more conservative, and the decision boundary will become smoother and smoother. This can help to reduce overfitting, but it can also cause the classifier to be less sensitive to subtle patterns in the training data. A larger value of k will lead to a less complex model, which is less prone to overfitting but more prone to underfitting.. Therefore, in order to avoid overfitting or underfitting, it is important to choose an appropriate value of k that strikes a balance between complexity and simplicity. It is usually better to try a range of values for the number of neighbors and see which one works best for a particular dataset."
 What will happen in the decision tree if the max depth is increased?," Increasing the max depth of a decision tree will increase the complexity of the model and make it more prone to overfitting. If you increase the max depth of a decision tree, the tree will be able to make more complex and nuanced decisions, which can improve the model’s ability to fit the training data well. However, if the tree is too deep, it may become overly sensitive to the specific patterns in the training data and not generalize well to unseen data."
 What is the difference between extra trees and random forests?," The main difference between the two algorithms is how the decision trees are constructed. In a Random Forest, the decision trees are constructed using bootstrapped samples of the training data and a random subset of the features. This results in each tree being trained on a slightly different set of data and features, leading to a greater diversity of trees and a lower variance. In an Extra Trees classifier, the decision trees are constructed in a similar way, but instead of selecting a random subset of the features at each split, the algorithm selects the best split among a random subset of the features. This results in a greater number of random splits and a higher degree of randomness, leading to a lower bias and a higher variance."
 When to use one-hot encoding and label encoding?," One-hot encoding and label encoding are two different techniques that can be used to encode categorical variables as numerical values. They are often used in machine learning models as a preprocessing step before fitting the model to the data. One-hot encoding is typically used when you have categorical variables that do not have any ordinal relationship, i.e., the categories do not have a natural order or ranking. One-hot encoding creates new binary columns for each category, with a value of 1 indicating the presence of the category and a value of 0 indicating the absence of the category. This can be useful when you want to preserve the uniqueness of each category and prevent the model from assuming any ordinal relationships between the categories. On the other hand, label encoding is typically used when you have categorical variables that do have an ordinal relationship, i.e., the categories have a natural order or ranking. Label encoding assigns a unique integer value to each category, and the integer values are usually determined by the natural order of the categories. This can be useful when you want to preserve the ordinal relationships between the categories and allow the model to make use of this information. In general, it is best to use one-hot encoding for nominal data (i.e., data that has no inherent order) and label encoding for ordinal data (i.e., data that has an inherent order). However, the choice between one-hot encoding and label encoding can also depend on the specific requirements of your model and the characteristics of your dataset."
 What is the problem with using label encoding for nominal data?," Label encoding is a method of encoding categorical variables as numerical values, which can be beneficial in certain situations. However, there are some potential problems that you should be aware of when using label encoding for nominal data. One problem with label encoding is that it can create an ordinal relationship between categories where none exists If you have a categorical variable with three categories: “red”, “green”, and “blue”, and you apply label encoding to map these categories to numerical values 0, 1, and 2, the model may assume that the category “green” is somehow “between” the categories “red” and “blue”. This can be a problem if your model depends on the assumption that the categories are independent of one another. Another problem with label encoding is that it can lead to unexpected results if you have an imbalanced dataset. For example, if one category is much more common than the others, it will be assigned a much lower numerical value, which could lead the model to give it less importance than it deserves."
 When can one-hot encoding be a problem?," One-hot encoding can be a problem in certain situations because it can create a large number of new columns in the dataset, which can make the data more difficult to work with and potentially lead to overfitting. One-hot encoding creates a new binary column for each category in a categorical variable. If you have a categorical variable with many categories, this can result in a very large number of new columns. Another problem with one-hot encoding is that it can lead to overfitting, especially if you have a small dataset and a large number of categories. When you create many new columns for each category, you are effectively increasing the number of features in the dataset. This can lead to overfitting, because the model may be able to memorize the training data, but it will not generalize well to new data. Finally, one-hot encoding can also be a problem if you need to add new categories to the dataset in the future. If you have already one-hot encoded the existing categories, you will need to be careful to ensure that the new categories are added in a way that does not create confusion or lead to unexpected results."
 What can be an appropriate encoding technique when you have hundreds of categorical values in a column?, A few techniques can be used when we have hundreds of columns in a categorical variable. Frequency encoding: This involves replacing each category with the frequency of that category in the dataset. This can work well if the categories have a natural ordinal relationship based on their frequency. Target encoding: This involves replacing each category with the mean of the target variable for that category. This can be effective if the categories have a clear relationship with the target variable.
 What are the sources of randomness in random forest ?," Random forests are an ensemble learning method that involves training multiple decision trees on different subsets of the data and averaging the predictions of the individual trees to make a final prediction. There are several sources of randomness in the process of training a random forest: Bootstrapped samples: When training each decision tree, the algorithm creates a bootstrapped sample of the data by sampling with replacement from the original training set. This means that some data points will be included in the sample multiple times, while others will not be included at all. This creates variation between the training sets of different trees. Random feature selection: When training each decision tree, the algorithm selects a random subset of the features to consider at each split. This means that different trees will consider different sets of features, leading to variation in the learned trees. Random threshold selection: When training each decision tree, the algorithm selects a random threshold for each feature to determine the optimal split. This means that different trees will split on different thresholds, leading to variation in the learned trees. By introducing these sources of randomness, random forests are able to reduce overfitting and improve generalization performance compared to a single decision tree."
 How do you decide which feature to split on at each node of the tree?," When training a decision tree, the algorithm must choose the feature to split on at each node of the tree. There are several strategies that can be used to decide which feature to split on, including: Greedy search: The algorithm selects the feature that maximizes a splitting criterion (such as information gain or Gini impurity) at each step. Random Search: The algorithm selects the feature to split on at random at each step. Exhaustive search: The algorithm considers all possible splits and selects the one that maximizes the splitting criterion. Forward search: The algorithm starts with an empty tree and adds splits one by one, selecting the split that maximizes the splitting criterion at each step. Backward search: The algorithm starts with a fully grown tree and prunes split one by one, selecting the split to remove that results in the smallest decrease in the splitting criterion."
 What is the significance of C in SVM?," In the support vector machine (SVM) algorithm, the parameter C is a hyperparameter that controls the trade-off between maximizing the margin and minimizing the misclassification error. Intuitively, C determines the penalty for misclassifying a training example. A smaller value of C means a larger penalty for misclassification, and therefore the model will try to correctly classify all training examples (even if it means a smaller margin). On the other hand, a larger value of C means a smaller penalty for misclassification, and therefore the model will try to maximize the margin even if it results in misclassifying some training examples. In practice, you can think of C as controlling the flexibility of the model. A smaller value of C will result in a more rigid model that may be more prone to underfitting, while a larger value of C will result in a more flexible model that may be more prone to overfitting. Therefore, the value of C should be chosen carefully using cross-validation, to balance the bias-variance trade-off and achieve good generalization performance on unseen data."
 How do c and gamma affect overfitting in SVM?," In support vector machines (SVMs), the regularization parameter C and the kernel parameter gamma are used to control overfitting. C is the penalty for misclassification. A smaller value of C means a larger penalty for misclassification, which means the model will be more conservative and try to avoid misclassification. This can lead to a model that is less prone to overfitting but may also result in a model that is too conservative and has poor generalization performance. Gamma is a parameter that controls the complexity of the model. A smaller value of gamma means a more complex model, which can lead to overfitting. A larger value of gamma means a simpler model, which can help prevent overfitting but may also result in a model that is too simple to accurately capture the underlying relationships in the data. In general, finding the optimal values for C and gamma is a trade-off between bias and variance, and it is often necessary to try different values and evaluate the model’s performance on a validation set to determine the best values for these parameters."
 How do you choose the number of models to use in a Boosting or Bagging ensemble?," The number of models to use in an ensemble is usually determined by the trade-off between performance and computational cost. As a general rule of thumb, increasing the number of models will improve the performance of the ensemble, but at the cost of increasing the computational cost. In practice, the number of models is determined by Cross validation which is used to determine the optimal number of models based on the evaluation metric chosen."
 In which scenarios Boosting and Bagging are preferred over single models?, Both boosting and bagging are generally preferred in scenarios where the individual models have high variance or high bias and the goal is to improve the overall performance of the model. Bagging is generally used to reduce the variance of a model while boosting is used to reduce bias and improve the generalization error of the model. Both methods are also useful when working with models that are sensitive to the training data and have a high chance of overfitting.
 Can you explain the ROC curve and AUC score and how they are used to evaluate a model’s performance?," A ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classification model. It plots the true positive rate (TPR) against the false positive rate (FPR) at different thresholds. AUC (Area Under the Curve) is the area under the ROC curve. It gives a single number that represents the model’s overall performance. AUC is useful because it considers all possible thresholds, not just a single point on the ROC curve."
 How do you approach setting the threshold in a binary classification problem when you want to adjust precision and recall by yourself?," When setting the threshold in a binary classification problem, it’s important to consider the trade-off between precision and recall. Precision is the proportion of true positive predictions out of all positive predictions, while recall is the proportion of true positive predictions out of all actual positive cases. One approach to adjusting precision and recall is to first train a model and then evaluate its performance on a validation set. The validation set should have a similar distribution of positive and negative cases as the test set on the model will be deployed. Next, you can use a confusion matrix to visualize the model’s performance and identify the current threshold that is being used to make predictions. A confusion matrix shows the number of true positive, false positive, true negative, and false negative predictions the model is making. From there, you can adjust the threshold to change the balance between precision and recall. For example, increasing the threshold will increase precision, but decrease recall. On the other hand, decreasing the threshold will increase recall and decrease precision. It is also important to consider the specific use case and the cost of false negatives and false positives. In certain applications, such as medical diagnosis, it may be more important to have a high recall (i.e., not to miss any actual positive cases) even if that means accepting a lower precision. In other cases, such as fraud detection, it may be more important to have high precision (i.e., not to flag any legitimate transactions as fraudulent) even if that means accepting a lower recall."
 What is the difference between LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis)?," LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) are both linear transformation techniques that are used to reduce the dimensionality of a dataset. However, they are used for different purposes and work in different ways. PCA is an unsupervised technique, which means it is used to find patterns in data without reference to any known labels. The goal of PCA is to find the directions (principal components) in the data that account for the greatest amount of variance. These directions are chosen such that they are mutually orthogonal (perpendicular) to each other, and the first direction accounts for the greatest variance, the second direction for the second greatest variance, and so on. Once the principal components are found, the data can be projected onto a lower-dimensional subspace defined by these components, resulting in a new, lower-dimensional representation of the data. LDA, on the other hand, is a supervised technique and is used to find a lower-dimensional subspace that maximizes the separation between different classes of data. LDA is commonly used as a dimensionality reduction technique for classification problems, for example in face recognition, iris recognition, and fingerprint recognition. The goal of LDA is to find a projection of the data that separates the classes as well as possible."
 How does the Naive Bayes algorithm compare to other supervised learning algorithms?," Naive Bayes is a simple and fast algorithm that works well with high-dimensional data and small training sets. It also performs well on datasets with categorical variables and missing data, which are common in many real-world problems. It is good for text classification, spam filtering, and sentiment analysis. However, due to the assumption of independence among features, it does not perform good for problems having high correlation among features. It also often fails to capture the interactions among features, which can result in poor performance on some datasets. Therefore, it is often used as a baseline or starting point, and then other algorithms like SVM, and Random Forest can be used to improve the performance."
 Can you explain the concept of the “kernel trick” and its application in Support Vector Machines (SVMs)?," The kernel trick is a technique used to transform the input data in SVMs to a higher-dimensional feature space, where it becomes linearly separable. The kernel trick works by replacing the standard inner product in the input space with a kernel function, which computes the inner product in a higher-dimensional space without actually having to compute the coordinates of the data in that space. This allows SVMs to handle non-linearly separable data by mapping it to a higher-dimensional space where it becomes linearly separable. Common kernel functions used in SVMs include the polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel. In this article, we covered various data science interview questions that cover topics such as KNN, linear regression, naive bayes, random forest, etc. The work of data scientists is not easy, but it is rewarding, and there are many open positions. These data science interview questions can get you one step closer to landing your ideal job. So, brace yourself for the rigors of interview questions and keep current on the fundamentals of data science. If you want to improve your data science skills, then consider signing up for our Blackbelt program."
